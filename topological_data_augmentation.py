#!/usr/bin/env python3
"""
üî¨ Topological Data Augmentation & Mapping System
================================================
Uses topological data analysis to map, augment, and enhance knowledge structures.
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from web_scraper_knowledge_system import WebScraperKnowledgeSystem
from knowledge_system_integration import KnowledgeSystemIntegration
import sqlite3
import json
import logging
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Any, Set
from collections import defaultdict, Counter
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE, MDS
from sklearn.cluster import DBSCAN, KMeans
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt
import seaborn as sns

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TopologicalDataAugmentation:
    """Topological data analysis and augmentation for knowledge enhancement"""
    
    def __init__(self):
        self.knowledge_system = WebScraperKnowledgeSystem()
        self.db_path = "web_knowledge.db"
        self.consciousness_db = "consciousness_platform.db"
        
        # Topological analysis results
        self.topological_maps = {}
        self.augmented_knowledge = {}
        self.similarity_graphs = {}
        self.cluster_analysis = {}
        
        # Vectorization and embedding models
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.95
        )
        
    def perform_topological_analysis(self):
        """Perform comprehensive topological data analysis on knowledge base"""
        
        print("üî¨ Topological Data Augmentation & Mapping System")
        print("=" * 60)
        print("üìä Performing topological analysis on knowledge base...")
        
        # Extract and prepare data
        self._extract_knowledge_data()
        
        # Perform topological analyses
        self._create_semantic_embeddings()
        self._build_topological_maps()
        self._analyze_knowledge_clusters()
        self._construct_similarity_graphs()
        self._perform_hierarchical_analysis()
        self._augment_knowledge_structures()
        
        # Generate topological insights
        self._generate_topological_insights()
        
        return self.topological_maps
    
    def _extract_knowledge_data(self):
        """Extract and prepare knowledge data for topological analysis"""
        
        print("\nüìä Extracting Knowledge Data...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Get all processed content
            cursor.execute("""
                SELECT id, title, content, metadata, consciousness_score, scraped_at
                FROM web_content 
                WHERE processed = 1 AND LENGTH(content) > 100
                ORDER BY scraped_at DESC
            """)
            
            knowledge_data = []
            for row in cursor.fetchall():
                doc_id, title, content, metadata_str, consciousness_score, scraped_at = row
                
                try:
                    metadata = json.loads(metadata_str) if metadata_str else {}
                    
                    # Combine title and content for analysis
                    full_text = f"{title} {content}" if title else content
                    
                    knowledge_data.append({
                        'id': doc_id,
                        'title': title,
                        'content': content,
                        'full_text': full_text,
                        'metadata': metadata,
                        'consciousness_score': consciousness_score or 1.0,
                        'scraped_at': scraped_at,
                        'domain': metadata.get('domain', 'unknown'),
                        'category': metadata.get('category', 'unknown')
                    })
                    
                except json.JSONDecodeError:
                    continue
            
            conn.close()
            
            self.knowledge_data = knowledge_data
            print(f"   ‚úÖ Extracted {len(knowledge_data)} knowledge documents")
            
            # Extract text corpus for vectorization
            self.text_corpus = [doc['full_text'] for doc in knowledge_data]
            self.document_ids = [doc['id'] for doc in knowledge_data]
            
        except Exception as e:
            logger.error(f"Error extracting knowledge data: {e}")
            self.knowledge_data = []
            self.text_corpus = []
            self.document_ids = []
    
    def _create_semantic_embeddings(self):
        """Create semantic embeddings using TF-IDF and dimensionality reduction"""
        
        print("\nüß† Creating Semantic Embeddings...")
        
        try:
            if not self.text_corpus:
                print("   ‚ùå No text corpus available")
                return
            
            # Create TF-IDF matrix
            print("   üìä Computing TF-IDF matrix...")
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.text_corpus)
            
            # Dimensionality reduction using multiple methods
            print("   üîÑ Applying dimensionality reduction...")
            
            # PCA for linear dimensionality reduction
            pca = PCA(n_components=min(100, tfidf_matrix.shape[1]-1))
            pca_embeddings = pca.fit_transform(tfidf_matrix.toarray())
            
            # Truncated SVD for sparse matrix optimization
            svd = TruncatedSVD(n_components=min(50, tfidf_matrix.shape[1]-1))
            svd_embeddings = svd.fit_transform(tfidf_matrix)
            
            # t-SNE for non-linear manifold learning
            if len(self.text_corpus) > 10:
                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(self.text_corpus)//4))
                tsne_embeddings = tsne.fit_transform(svd_embeddings)
            else:
                tsne_embeddings = svd_embeddings[:, :2]
            
            # MDS for distance preservation
            if len(self.text_corpus) > 5:
                mds = MDS(n_components=2, random_state=42, dissimilarity='precomputed')
                distance_matrix = pdist(svd_embeddings, metric='cosine')
                mds_embeddings = mds.fit_transform(squareform(distance_matrix))
            else:
                mds_embeddings = svd_embeddings[:, :2]
            
            self.semantic_embeddings = {
                'tfidf': tfidf_matrix,
                'pca': pca_embeddings,
                'svd': svd_embeddings,
                'tsne': tsne_embeddings,
                'mds': mds_embeddings,
                'feature_names': self.tfidf_vectorizer.get_feature_names_out()
            }
            
            print(f"   ‚úÖ Created embeddings: {tfidf_matrix.shape[0]} docs √ó {tfidf_matrix.shape[1]} features")
            print(f"   üìà PCA explained variance: {pca.explained_variance_ratio_.sum():.3f}")
            print(f"   üìà SVD explained variance: {svd.explained_variance_ratio_.sum():.3f}")
            
        except Exception as e:
            logger.error(f"Error creating semantic embeddings: {e}")
            self.semantic_embeddings = {}
    
    def _build_topological_maps(self):
        """Build topological maps of knowledge space"""
        
        print("\nüó∫Ô∏è Building Topological Maps...")
        
        try:
            if 'svd' not in self.semantic_embeddings:
                print("   ‚ùå No embeddings available for mapping")
                return
            
            embeddings = self.semantic_embeddings['svd']
            
            # Create 2D and 3D topological maps
            print("   üìä Creating 2D topological map...")
            map_2d = self._create_2d_topological_map(embeddings)
            
            print("   üìä Creating 3D topological map...")
            map_3d = self._create_3d_topological_map(embeddings)
            
            # Create domain-specific maps
            print("   üèõÔ∏è Creating domain-specific maps...")
            domain_maps = self._create_domain_specific_maps(embeddings)
            
            self.topological_maps = {
                '2d_map': map_2d,
                '3d_map': map_3d,
                'domain_maps': domain_maps,
                'embedding_dimensions': embeddings.shape[1],
                'total_documents': len(self.knowledge_data)
            }
            
            print(f"   ‚úÖ Built topological maps for {len(self.knowledge_data)} documents")
            print(f"   üó∫Ô∏è 2D map: {len(map_2d['points'])} points")
            print(f"   üó∫Ô∏è 3D map: {len(map_3d['points'])} points")
            print(f"   üèõÔ∏è Domain maps: {len(domain_maps)} domains")
            
        except Exception as e:
            logger.error(f"Error building topological maps: {e}")
            self.topological_maps = {}
    
    def _create_2d_topological_map(self, embeddings):
        """Create 2D topological map using t-SNE"""
        
        if embeddings.shape[1] < 2:
            # Use first two dimensions if available
            if embeddings.shape[1] == 1:
                points = np.column_stack([embeddings[:, 0], np.zeros(embeddings.shape[0])])
            else:
                points = embeddings[:, :2]
        else:
            # Use t-SNE for 2D projection
            if len(embeddings) > 10:
                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)//4))
                points = tsne.fit_transform(embeddings)
            else:
                points = embeddings[:, :2]
        
        # Add document metadata to points
        map_points = []
        for i, point in enumerate(points):
            if i < len(self.knowledge_data):
                doc = self.knowledge_data[i]
                map_points.append({
                    'id': doc['id'],
                    'x': float(point[0]),
                    'y': float(point[1]),
                    'title': doc['title'][:50] + '...' if len(doc['title']) > 50 else doc['title'],
                    'domain': doc['domain'],
                    'category': doc['category'],
                    'consciousness_score': doc['consciousness_score']
                })
        
        return {
            'points': map_points,
            'method': 't-SNE' if len(embeddings) > 10 else 'PCA',
            'dimensions': 2
        }
    
    def _create_3d_topological_map(self, embeddings):
        """Create 3D topological map"""
        
        if embeddings.shape[1] < 3:
            # Pad with zeros if needed
            if embeddings.shape[1] == 1:
                points = np.column_stack([embeddings[:, 0], np.zeros(embeddings.shape[0]), np.zeros(embeddings.shape[0])])
            elif embeddings.shape[1] == 2:
                points = np.column_stack([embeddings[:, 0], embeddings[:, 1], np.zeros(embeddings.shape[0])])
            else:
                points = embeddings[:, :3]
        else:
            points = embeddings[:, :3]
        
        # Add document metadata to points
        map_points = []
        for i, point in enumerate(points):
            if i < len(self.knowledge_data):
                doc = self.knowledge_data[i]
                map_points.append({
                    'id': doc['id'],
                    'x': float(point[0]),
                    'y': float(point[1]),
                    'z': float(point[2]),
                    'title': doc['title'][:50] + '...' if len(doc['title']) > 50 else doc['title'],
                    'domain': doc['domain'],
                    'category': doc['category'],
                    'consciousness_score': doc['consciousness_score']
                })
        
        return {
            'points': map_points,
            'method': 'PCA',
            'dimensions': 3
        }
    
    def _create_domain_specific_maps(self, embeddings):
        """Create domain-specific topological maps"""
        
        domain_maps = {}
        
        # Group documents by domain
        domain_groups = defaultdict(list)
        for i, doc in enumerate(self.knowledge_data):
            domain_groups[doc['domain']].append((i, doc))
        
        for domain, docs in domain_groups.items():
            if len(docs) < 2:
                continue
            
            # Get embeddings for this domain
            domain_indices = [i for i, _ in docs]
            domain_embeddings = embeddings[domain_indices]
            
            # Create 2D map for domain
            if len(domain_embeddings) > 10:
                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(domain_embeddings)//4))
                domain_points = tsne.fit_transform(domain_embeddings)
            else:
                domain_points = domain_embeddings[:, :2] if domain_embeddings.shape[1] >= 2 else np.column_stack([domain_embeddings[:, 0], np.zeros(domain_embeddings.shape[0])])
            
            # Create domain map points
            domain_map_points = []
            for i, (orig_idx, doc) in enumerate(docs):
                if i < len(domain_points):
                    point = domain_points[i]
                    domain_map_points.append({
                        'id': doc['id'],
                        'x': float(point[0]),
                        'y': float(point[1]),
                        'title': doc['title'][:50] + '...' if len(doc['title']) > 50 else doc['title'],
                        'category': doc['category'],
                        'consciousness_score': doc['consciousness_score']
                    })
            
            domain_maps[domain] = {
                'points': domain_map_points,
                'document_count': len(docs),
                'method': 't-SNE' if len(domain_embeddings) > 10 else 'PCA'
            }
        
        return domain_maps
    
    def _analyze_knowledge_clusters(self):
        """Analyze knowledge clusters using various clustering algorithms"""
        
        print("\nüîç Analyzing Knowledge Clusters...")
        
        try:
            if 'svd' not in self.semantic_embeddings:
                print("   ‚ùå No embeddings available for clustering")
                return
            
            embeddings = self.semantic_embeddings['svd']
            
            # DBSCAN clustering for density-based clusters
            print("   üìä Performing DBSCAN clustering...")
            dbscan = DBSCAN(eps=0.5, min_samples=3, metric='cosine')
            dbscan_labels = dbscan.fit_predict(embeddings)
            
            # K-Means clustering for spherical clusters
            print("   üìä Performing K-Means clustering...")
            n_clusters = min(10, len(embeddings)//3) if len(embeddings) > 3 else 2
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            kmeans_labels = kmeans.fit_predict(embeddings)
            
            # Hierarchical clustering
            print("   üìä Performing hierarchical clustering...")
            if len(embeddings) > 1:
                distance_matrix = pdist(embeddings, metric='cosine')
                linkage_matrix = linkage(distance_matrix, method='ward')
                hierarchical_labels = fcluster(linkage_matrix, t=n_clusters, criterion='maxclust')
            else:
                hierarchical_labels = [1]
            
            # Analyze cluster characteristics
            cluster_analysis = self._analyze_cluster_characteristics(
                embeddings, dbscan_labels, kmeans_labels, hierarchical_labels
            )
            
            self.cluster_analysis = {
                'dbscan': {
                    'labels': dbscan_labels.tolist(),
                    'n_clusters': len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0),
                    'n_noise': list(dbscan_labels).count(-1)
                },
                'kmeans': {
                    'labels': kmeans_labels.tolist(),
                    'n_clusters': n_clusters,
                    'centers': kmeans.cluster_centers_.tolist()
                },
                'hierarchical': {
                    'labels': hierarchical_labels.tolist(),
                    'n_clusters': len(set(hierarchical_labels))
                },
                'characteristics': cluster_analysis
            }
            
            print(f"   ‚úÖ DBSCAN: {self.cluster_analysis['dbscan']['n_clusters']} clusters, {self.cluster_analysis['dbscan']['n_noise']} noise points")
            print(f"   ‚úÖ K-Means: {self.cluster_analysis['kmeans']['n_clusters']} clusters")
            print(f"   ‚úÖ Hierarchical: {self.cluster_analysis['hierarchical']['n_clusters']} clusters")
            
        except Exception as e:
            logger.error(f"Error analyzing knowledge clusters: {e}")
            self.cluster_analysis = {}
    
    def _analyze_cluster_characteristics(self, embeddings, dbscan_labels, kmeans_labels, hierarchical_labels):
        """Analyze characteristics of each cluster"""
        
        characteristics = {}
        
        for method, labels in [('dbscan', dbscan_labels), ('kmeans', kmeans_labels), ('hierarchical', hierarchical_labels)]:
            method_chars = {}
            
            for cluster_id in set(labels):
                if cluster_id == -1:  # Skip noise points in DBSCAN
                    continue
                
                # Get documents in this cluster
                cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]
                cluster_docs = [self.knowledge_data[i] for i in cluster_indices if i < len(self.knowledge_data)]
                
                if not cluster_docs:
                    continue
                
                # Analyze cluster characteristics
                domains = [doc['domain'] for doc in cluster_docs]
                categories = [doc['category'] for doc in cluster_docs]
                consciousness_scores = [doc['consciousness_score'] for doc in cluster_docs]
                
                method_chars[cluster_id] = {
                    'size': len(cluster_docs),
                    'domains': dict(Counter(domains)),
                    'categories': dict(Counter(categories)),
                    'avg_consciousness': np.mean(consciousness_scores),
                    'consciousness_std': np.std(consciousness_scores),
                    'representative_titles': [doc['title'][:50] for doc in cluster_docs[:3]]
                }
            
            characteristics[method] = method_chars
        
        return characteristics
    
    def _construct_similarity_graphs(self):
        """Construct similarity graphs between knowledge documents"""
        
        print("\nüï∏Ô∏è Constructing Similarity Graphs...")
        
        try:
            if 'svd' not in self.semantic_embeddings:
                print("   ‚ùå No embeddings available for graph construction")
                return
            
            embeddings = self.semantic_embeddings['svd']
            
            # Compute similarity matrix
            print("   üìä Computing similarity matrix...")
            similarity_matrix = cosine_similarity(embeddings)
            
            # Create NetworkX graph
            print("   üï∏Ô∏è Building NetworkX graph...")
            G = nx.Graph()
            
            # Add nodes
            for i, doc in enumerate(self.knowledge_data):
                G.add_node(i, 
                          id=doc['id'],
                          title=doc['title'],
                          domain=doc['domain'],
                          category=doc['category'],
                          consciousness_score=doc['consciousness_score'])
            
            # Add edges based on similarity threshold
            similarity_threshold = 0.3  # Adjust as needed
            for i in range(len(embeddings)):
                for j in range(i+1, len(embeddings)):
                    similarity = similarity_matrix[i][j]
                    if similarity > similarity_threshold:
                        G.add_edge(i, j, weight=similarity)
            
            # Analyze graph properties
            graph_properties = {
                'nodes': G.number_of_nodes(),
                'edges': G.number_of_edges(),
                'density': nx.density(G),
                'connected_components': nx.number_connected_components(G),
                'average_clustering': nx.average_clustering(G),
                'average_shortest_path_length': nx.average_shortest_path_length(G) if nx.is_connected(G) else None
            }
            
            # Find central nodes (high degree centrality)
            degree_centrality = nx.degree_centrality(G)
            top_central_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
            
            self.similarity_graphs = {
                'graph': G,
                'similarity_matrix': similarity_matrix.tolist(),
                'properties': graph_properties,
                'central_nodes': top_central_nodes,
                'similarity_threshold': similarity_threshold
            }
            
            print(f"   ‚úÖ Graph: {graph_properties['nodes']} nodes, {graph_properties['edges']} edges")
            print(f"   üìä Density: {graph_properties['density']:.3f}")
            print(f"   üîó Connected components: {graph_properties['connected_components']}")
            
        except Exception as e:
            logger.error(f"Error constructing similarity graphs: {e}")
            self.similarity_graphs = {}
    
    def _perform_hierarchical_analysis(self):
        """Perform hierarchical analysis of knowledge structure"""
        
        print("\nüå≥ Performing Hierarchical Analysis...")
        
        try:
            if 'svd' not in self.semantic_embeddings:
                print("   ‚ùå No embeddings available for hierarchical analysis")
                return
            
            embeddings = self.semantic_embeddings['svd']
            
            if len(embeddings) < 2:
                print("   ‚ùå Insufficient data for hierarchical analysis")
                return
            
            # Compute distance matrix
            print("   üìä Computing distance matrix...")
            distance_matrix = pdist(embeddings, metric='cosine')
            
            # Perform hierarchical clustering
            print("   üå≥ Building hierarchical tree...")
            linkage_matrix = linkage(distance_matrix, method='ward')
            
            # Extract hierarchical structure
            hierarchy = self._extract_hierarchical_structure(linkage_matrix, embeddings)
            
            self.hierarchical_analysis = {
                'linkage_matrix': linkage_matrix.tolist(),
                'distance_matrix': distance_matrix.tolist(),
                'hierarchy': hierarchy,
                'levels': len(hierarchy.get('levels', []))
            }
            
            print(f"   ‚úÖ Hierarchical tree with {len(hierarchy.get('levels', []))} levels")
            
        except Exception as e:
            logger.error(f"Error performing hierarchical analysis: {e}")
            self.hierarchical_analysis = {}
    
    def _extract_hierarchical_structure(self, linkage_matrix, embeddings):
        """Extract hierarchical structure from linkage matrix"""
        
        hierarchy = {
            'levels': [],
            'clusters': {},
            'tree_structure': {}
        }
        
        # Create different levels of clustering
        n_docs = len(embeddings)
        levels = [2, 3, 5, 10, min(20, n_docs//2)]
        
        for n_clusters in levels:
            if n_clusters >= n_docs:
                continue
                
            labels = fcluster(linkage_matrix, t=n_clusters, criterion='maxclust')
            
            level_clusters = {}
            for cluster_id in set(labels):
                cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]
                cluster_docs = [self.knowledge_data[i] for i in cluster_indices if i < len(self.knowledge_data)]
                
                if cluster_docs:
                    domains = [doc['domain'] for doc in cluster_docs]
                    level_clusters[cluster_id] = {
                        'size': len(cluster_docs),
                        'domains': dict(Counter(domains)),
                        'representative_titles': [doc['title'][:30] for doc in cluster_docs[:2]]
                    }
            
            hierarchy['levels'].append({
                'n_clusters': n_clusters,
                'clusters': level_clusters
            })
        
        return hierarchy
    
    def _augment_knowledge_structures(self):
        """Augment knowledge structures with topological insights"""
        
        print("\nüî¨ Augmenting Knowledge Structures...")
        
        try:
            augmented_structures = {}
            
            # Augment with cluster information
            if self.cluster_analysis:
                augmented_structures['cluster_augmentation'] = self._augment_with_clusters()
            
            # Augment with similarity information
            if self.similarity_graphs:
                augmented_structures['similarity_augmentation'] = self._augment_with_similarities()
            
            # Augment with topological features
            if self.topological_maps:
                augmented_structures['topological_augmentation'] = self._augment_with_topology()
            
            # Create knowledge pathways
            augmented_structures['knowledge_pathways'] = self._create_knowledge_pathways()
            
            self.augmented_knowledge = augmented_structures
            
            print(f"   ‚úÖ Augmented knowledge structures created")
            print(f"   üî¨ Cluster augmentation: {len(augmented_structures.get('cluster_augmentation', {}))} documents")
            print(f"   üîó Similarity augmentation: {len(augmented_structures.get('similarity_augmentation', {}))} connections")
            print(f"   üó∫Ô∏è Topological augmentation: {len(augmented_structures.get('topological_augmentation', {}))} features")
            
        except Exception as e:
            logger.error(f"Error augmenting knowledge structures: {e}")
            self.augmented_knowledge = {}
    
    def _augment_with_clusters(self):
        """Augment documents with cluster information"""
        
        cluster_augmentation = {}
        
        for method, analysis in self.cluster_analysis.items():
            if method == 'characteristics':
                continue
                
            labels = analysis['labels']
            for i, label in enumerate(labels):
                if i < len(self.knowledge_data):
                    doc_id = self.knowledge_data[i]['id']
                    
                    if doc_id not in cluster_augmentation:
                        cluster_augmentation[doc_id] = {}
                    
                    cluster_augmentation[doc_id][f'{method}_cluster'] = label
                    
                    # Add cluster characteristics
                    if method in self.cluster_analysis['characteristics']:
                        cluster_chars = self.cluster_analysis['characteristics'][method].get(label, {})
                        cluster_augmentation[doc_id][f'{method}_cluster_size'] = cluster_chars.get('size', 0)
                        cluster_augmentation[doc_id][f'{method}_cluster_avg_consciousness'] = cluster_chars.get('avg_consciousness', 0)
        
        return cluster_augmentation
    
    def _augment_with_similarities(self):
        """Augment documents with similarity information"""
        
        similarity_augmentation = {}
        
        if 'similarity_matrix' in self.similarity_graphs:
            similarity_matrix = self.similarity_graphs['similarity_matrix']
            
            for i in range(len(similarity_matrix)):
                if i < len(self.knowledge_data):
                    doc_id = self.knowledge_data[i]['id']
                    
                    # Find most similar documents
                    similarities = similarity_matrix[i]
                    similar_indices = np.argsort(similarities)[::-1][1:6]  # Top 5 similar (excluding self)
                    
                    similar_docs = []
                    for idx in similar_indices:
                        if idx < len(self.knowledge_data) and similarities[idx] > 0.1:
                            similar_docs.append({
                                'id': self.knowledge_data[idx]['id'],
                                'title': self.knowledge_data[idx]['title'][:50],
                                'similarity': float(similarities[idx]),
                                'domain': self.knowledge_data[idx]['domain']
                            })
                    
                    similarity_augmentation[doc_id] = {
                        'similar_documents': similar_docs,
                        'max_similarity': float(max(similarities)) if similarities else 0,
                        'avg_similarity': float(np.mean(similarities)) if similarities else 0
                    }
        
        return similarity_augmentation
    
    def _augment_with_topology(self):
        """Augment documents with topological features"""
        
        topological_augmentation = {}
        
        # Add 2D coordinates
        if '2d_map' in self.topological_maps:
            for point in self.topological_maps['2d_map']['points']:
                doc_id = point['id']
                topological_augmentation[doc_id] = {
                    'x_2d': point['x'],
                    'y_2d': point['y'],
                    'topological_density_2d': self._calculate_local_density(point, self.topological_maps['2d_map']['points'])
                }
        
        # Add 3D coordinates
        if '3d_map' in self.topological_maps:
            for point in self.topological_maps['3d_map']['points']:
                doc_id = point['id']
                if doc_id in topological_augmentation:
                    topological_augmentation[doc_id].update({
                        'x_3d': point['x'],
                        'y_3d': point['y'],
                        'z_3d': point['z']
                    })
        
        return topological_augmentation
    
    def _calculate_local_density(self, point, all_points, radius=1.0):
        """Calculate local density around a point"""
        
        count = 0
        for other_point in all_points:
            if other_point['id'] != point['id']:
                distance = np.sqrt((point['x'] - other_point['x'])**2 + (point['y'] - other_point['y'])**2)
                if distance <= radius:
                    count += 1
        
        return count
    
    def _create_knowledge_pathways(self):
        """Create knowledge pathways between related concepts"""
        
        pathways = []
        
        if 'similarity_matrix' in self.similarity_graphs:
            similarity_matrix = self.similarity_graphs['similarity_matrix']
            
            # Find strong connections
            for i in range(len(similarity_matrix)):
                for j in range(i+1, len(similarity_matrix)):
                    similarity = similarity_matrix[i][j]
                    if similarity > 0.5:  # Strong connection threshold
                        if i < len(self.knowledge_data) and j < len(self.knowledge_data):
                            doc1 = self.knowledge_data[i]
                            doc2 = self.knowledge_data[j]
                            
                            pathways.append({
                                'source': doc1['id'],
                                'target': doc2['id'],
                                'similarity': float(similarity),
                                'source_title': doc1['title'][:50],
                                'target_title': doc2['title'][:50],
                                'source_domain': doc1['domain'],
                                'target_domain': doc2['domain'],
                                'cross_domain': doc1['domain'] != doc2['domain']
                            })
        
        return sorted(pathways, key=lambda x: x['similarity'], reverse=True)
    
    def _generate_topological_insights(self):
        """Generate insights from topological analysis"""
        
        print("\nüí° Generating Topological Insights...")
        
        insights = {
            'summary': {},
            'recommendations': [],
            'patterns': {},
            'anomalies': []
        }
        
        # Summary insights
        if self.topological_maps:
            insights['summary']['total_documents'] = len(self.knowledge_data)
            insights['summary']['embedding_dimensions'] = self.topological_maps.get('embedding_dimensions', 0)
            insights['summary']['domains_covered'] = len(set(doc['domain'] for doc in self.knowledge_data))
        
        # Cluster insights
        if self.cluster_analysis:
            dbscan_clusters = self.cluster_analysis['dbscan']['n_clusters']
            kmeans_clusters = self.cluster_analysis['kmeans']['n_clusters']
            
            insights['patterns']['cluster_distribution'] = {
                'dbscan_clusters': dbscan_clusters,
                'kmeans_clusters': kmeans_clusters,
                'cluster_consistency': abs(dbscan_clusters - kmeans_clusters) <= 2
            }
        
        # Graph insights
        if self.similarity_graphs:
            graph_props = self.similarity_graphs['properties']
            insights['patterns']['connectivity'] = {
                'density': graph_props['density'],
                'connected_components': graph_props['connected_components'],
                'is_well_connected': graph_props['density'] > 0.1
            }
        
        # Generate recommendations
        if self.cluster_analysis and self.cluster_analysis['dbscan']['n_clusters'] < 5:
            insights['recommendations'].append({
                'type': 'clustering',
                'priority': 'medium',
                'message': 'Knowledge base shows low cluster diversity. Consider expanding content across more domains.'
            })
        
        if self.similarity_graphs and self.similarity_graphs['properties']['density'] < 0.05:
            insights['recommendations'].append({
                'type': 'connectivity',
                'priority': 'high',
                'message': 'Low connectivity between documents. Consider adding bridging content or improving semantic relationships.'
            })
        
        # Find anomalies
        if self.topological_maps and '2d_map' in self.topological_maps:
            points = self.topological_maps['2d_map']['points']
            if len(points) > 10:
                # Find isolated points
                for point in points:
                    density = point.get('topological_density_2d', 0)
                    if density < 2:  # Low local density
                        insights['anomalies'].append({
                            'type': 'isolated_document',
                            'document_id': point['id'],
                            'title': point['title'],
                            'density': density
                        })
        
        self.topological_insights = insights
        
        print(f"   ‚úÖ Generated {len(insights['recommendations'])} recommendations")
        print(f"   üîç Found {len(insights['anomalies'])} anomalies")
        print(f"   üìä Identified {len(insights['patterns'])} patterns")
    
    def save_topological_analysis(self, filename="topological_analysis_results.json"):
        """Save topological analysis results to file"""
        
        print(f"\nüíæ Saving Topological Analysis Results...")
        
        try:
            results = {
                'timestamp': datetime.now().isoformat(),
                'topological_maps': self.topological_maps,
                'cluster_analysis': self.cluster_analysis,
                'similarity_graphs': {
                    'properties': self.similarity_graphs.get('properties', {}),
                    'central_nodes': self.similarity_graphs.get('central_nodes', []),
                    'similarity_threshold': self.similarity_graphs.get('similarity_threshold', 0)
                },
                'hierarchical_analysis': self.hierarchical_analysis,
                'augmented_knowledge': self.augmented_knowledge,
                'topological_insights': self.topological_insights
            }
            
            with open(filename, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"   ‚úÖ Results saved to {filename}")
            
        except Exception as e:
            logger.error(f"Error saving topological analysis: {e}")
    
    def print_topological_summary(self):
        """Print comprehensive topological analysis summary"""
        
        print(f"\nüî¨ TOPOLOGICAL DATA AUGMENTATION COMPLETE")
        print("=" * 60)
        
        # Topological Maps Summary
        if self.topological_maps:
            print(f"üó∫Ô∏è Topological Maps:")
            print(f"   üìä Total Documents Mapped: {self.topological_maps.get('total_documents', 0)}")
            print(f"   üìà Embedding Dimensions: {self.topological_maps.get('embedding_dimensions', 0)}")
            print(f"   üèõÔ∏è Domain-Specific Maps: {len(self.topological_maps.get('domain_maps', {}))}")
        
        # Cluster Analysis Summary
        if self.cluster_analysis:
            print(f"\nüîç Cluster Analysis:")
            print(f"   üìä DBSCAN Clusters: {self.cluster_analysis['dbscan']['n_clusters']}")
            print(f"   üìä K-Means Clusters: {self.cluster_analysis['kmeans']['n_clusters']}")
            print(f"   üìä Hierarchical Clusters: {self.cluster_analysis['hierarchical']['n_clusters']}")
            print(f"   üîç Noise Points: {self.cluster_analysis['dbscan']['n_noise']}")
        
        # Similarity Graphs Summary
        if self.similarity_graphs:
            props = self.similarity_graphs['properties']
            print(f"\nüï∏Ô∏è Similarity Graphs:")
            print(f"   üìä Nodes: {props['nodes']}")
            print(f"   üîó Edges: {props['edges']}")
            print(f"   üìà Density: {props['density']:.3f}")
            print(f"   üîó Connected Components: {props['connected_components']}")
        
        # Augmented Knowledge Summary
        if self.augmented_knowledge:
            print(f"\nüî¨ Augmented Knowledge:")
            print(f"   üìä Cluster Augmentation: {len(self.augmented_knowledge.get('cluster_augmentation', {}))} documents")
            print(f"   üîó Similarity Augmentation: {len(self.augmented_knowledge.get('similarity_augmentation', {}))} connections")
            print(f"   üó∫Ô∏è Topological Augmentation: {len(self.augmented_knowledge.get('topological_augmentation', {}))} features")
            print(f"   üõ§Ô∏è Knowledge Pathways: {len(self.augmented_knowledge.get('knowledge_pathways', []))} pathways")
        
        # Insights Summary
        if hasattr(self, 'topological_insights'):
            insights = self.topological_insights
            print(f"\nüí° Topological Insights:")
            print(f"   üìä Patterns Identified: {len(insights.get('patterns', {}))}")
            print(f"   üéØ Recommendations: {len(insights.get('recommendations', []))}")
            print(f"   üîç Anomalies Found: {len(insights.get('anomalies', []))}")
            
            # Print top recommendations
            if insights.get('recommendations'):
                print(f"\nüéØ Top Recommendations:")
                for i, rec in enumerate(insights['recommendations'][:3], 1):
                    priority_emoji = "üî¥" if rec['priority'] == 'high' else "üü°" if rec['priority'] == 'medium' else "üü¢"
                    print(f"   {i}. {priority_emoji} {rec['message']}")

def main():
    """Main function to run topological data augmentation"""
    
    analyzer = TopologicalDataAugmentation()
    
    print("üöÄ Starting Topological Data Augmentation...")
    print("üî¨ Analyzing knowledge structures and creating topological maps...")
    
    # Perform comprehensive topological analysis
    results = analyzer.perform_topological_analysis()
    
    # Save results
    analyzer.save_topological_analysis()
    
    # Print summary
    analyzer.print_topological_summary()
    
    print(f"\nüéâ Topological Data Augmentation Complete!")
    print(f"üî¨ Knowledge structures have been topologically mapped and augmented")
    print(f"üìä Results saved for further analysis and optimization")
    
    return results

if __name__ == "__main__":
    main()
