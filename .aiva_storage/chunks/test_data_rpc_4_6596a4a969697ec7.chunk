    assert pending_root is not None

        if layer == InterfaceLayer.direct:
            cleared_root = await data_rpc_api.clear_pending_roots({"store_id": store_id.hex()})
        elif layer == InterfaceLayer.funcs:
            cleared_root = await clear_pending_roots(
                store_id=store_id,
                rpc_port=rpc_port,
                root_path=bt.root_path,
            )
        elif layer == InterfaceLayer.cli:
            args: list[str] = [
                sys.executable,
                "-m",
                "chia",
                "data",
                "clear_pending_roots",
                "--id",
                store_id.hex(),
                "--data-rpc-port",
                str(rpc_port),
                "--yes",
            ]
            process = await asyncio.create_subprocess_exec(
                *args,
                env={**os.environ, "CHIA_ROOT": str(bt.root_path)},
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            await process.wait()
            assert process.stdout is not None
            assert process.stderr is not None
            stdout = await process.stdout.read()
            cleared_root = json.loads(stdout)
            stderr = await process.stderr.read()
            assert process.returncode == 0
            if sys.version_info >= (3, 10, 6):
                assert stderr == b""
            else:  # pragma: no cover
                # https://github.com/python/cpython/issues/92841
                assert stderr == b"" or b"_ProactorBasePipeTransport.__del__" in stderr
        elif layer == InterfaceLayer.client:
            async with DataLayerRpcClient.create_as_context(
                self_hostname=self_hostname,
                port=rpc_port,
                root_path=bt.root_path,
                net_config=bt.config,
            ) as client:
                cleared_root = await client.clear_pending_roots(store_id=store_id)
        else:  # pragma: no cover
            assert False, "unhandled parametrization"

        assert cleared_root == {"success": True, "root": pending_root.marshal()}


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_issue_15955_deadlock(
    self_hostname: str, one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices, tmp_path: Path
) -> None:
    wallet_rpc_api, full_node_api, wallet_rpc_port, _ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )

    wallet_node = wallet_rpc_api.service
    wallet = wallet_node.wallet_state_manager.main_wallet

    interval = 1
    config = bt.config
    config["data_layer"]["manage_data_interval"] = interval
    bt.change_config(new_config=config)

    async with init_data_layer(wallet_rpc_port=wallet_rpc_port, bt=bt, db_path=tmp_path) as data_layer:
        # get some xch
        await full_node_api.farm_blocks_to_wallet(count=1, wallet=wallet)
        await full_node_api.wait_for_wallet_synced(wallet_node)

        # create a store
        transaction_records, store_id = await data_layer.create_store(fee=uint64(0))
        await full_node_api.process_transaction_records(records=transaction_records)
        await full_node_api.wait_for_wallet_synced(wallet_node)
        assert await check_singleton_confirmed(dl=data_layer, store_id=store_id)

        # insert a key and value
        key = b"\x00"
        value = b"\x01" * 10_000
        transaction_record = await data_layer.batch_update(
            store_id=store_id,
            changelist=[{"action": "insert", "key": key, "value": value}],
            fee=uint64(0),
        )
        assert transaction_record is not None
        await full_node_api.process_transaction_records(records=[transaction_record])
        await full_node_api.wait_for_wallet_synced(wallet_node)
        assert await check_singleton_confirmed(dl=data_layer, store_id=store_id)

        # get the value a bunch through several periodic data management cycles
        concurrent_requests = 10
        time_per_request = 2
        timeout = concurrent_requests * time_per_request

        duration = 10 * interval
        start = time.monotonic()
        end = start + duration

        while time.monotonic() < end:
            with anyio.fail_after(adjusted_timeout(timeout)):
                await asyncio.gather(
                    *(create_referenced_task(data_layer.get_value(store_id=store_id, key=key)) for _ in range(10))
                )


@pytest.mark.parametrize(argnames="maximum_full_file_count", argvalues=[1, 5, 100])
@boolean_datacases(name="group_files_by_store", false="group by singleton", true="don't group by singleton")
@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_maximum_full_file_count(
    self_hostname: str,
    one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices,
    tmp_path: Path,
    maximum_full_file_count: int,
    group_files_by_store: bool,
) -> None:
    wallet_rpc_api, full_node_api, wallet_rpc_port, ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )
    manage_data_interval = 5
    async with init_data_layer(
        wallet_rpc_port=wallet_rpc_port,
        bt=bt,
        db_path=tmp_path,
        manage_data_interval=manage_data_interval,
        maximum_full_file_count=maximum_full_file_count,
        group_files_by_store=group_files_by_store,
    ) as data_layer:
        data_rpc_api = DataLayerRpcApi(data_layer)
        res = await data_rpc_api.create_data_store({})
        root_hashes: list[bytes32] = []
        assert res is not None
        store_id = bytes32.from_hexstr(res["id"])
        await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)
        await full_node_api.wait_for_wallet_synced(wallet_node=wallet_rpc_api.service, timeout=20)
        for batch_count in range(1, 10):
            key = batch_count.to_bytes(2, "big")
            value = batch_count.to_bytes(2, "big")
            changelist = [{"action": "insert", "key": key.hex(), "value": value.hex()}]
            res = await data_rpc_api.batch_update({"id": store_id.hex(), "changelist": changelist})
            update_tx_rec = res["tx_id"]
            await farm_block_with_spend(full_node_api, ph, update_tx_rec, wallet_rpc_api)
            await asyncio.sleep(manage_data_interval * 2)
            root_hash = await data_rpc_api.get_root({"id": store_id.hex()})
            root_hashes.append(root_hash["hash"])
            expected_files_count = min(batch_count, maximum_full_file_count) + batch_count
            server_files_location = (
                data_layer.server_files_location.joinpath(f"{store_id}")
                if group_files_by_store
                else data_layer.server_files_location
            )
            with os.scandir(server_files_location) as entries:
                filenames = {entry.name for entry in entries}
                assert len(filenames) == expected_files_count
            for generation, hash in enumerate(root_hashes):
                delta_path = get_delta_filename_path(
                    data_layer.server_files_location,
                    store_id,
                    hash,
                    generation + 1,
                    group_files_by_store,
                )
                assert delta_path.exists()
                full_file_path = get_full_tree_filename_path(
                    data_layer.server_files_location,
                    store_id,
                    hash,
                    generation + 1,
                    group_files_by_store,
                )
                if generation + 1 > batch_count - maximum_full_file_count:
                    assert full_file_path.exists()
                else:
                    assert not full_file_path.exists()


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_unsubscribe_unknown(
    bare_data_layer_api: DataLayerRpcApi,
    seeded_random: random.Random,
) -> None:
    with pytest.raises(RuntimeError, match="No subscription found for the given store_id"):
        await bare_data_layer_api.unsubscribe(request={"id": bytes32.random(seeded_random).hex(), "retain": False})


@pytest.mark.parametrize("retain", [True, False])
@boolean_datacases(name="group_files_by_store", false="group by singleton", true="don't group by singleton")
@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_unsubscribe_removes_files(
    self_hostname: str,
    one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices,
    tmp_path: Path,
    retain: bool,
    group_files_by_store: bool,
) -> None:
    wallet_rpc_api, full_node_api, wallet_rpc_port, ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )
    manage_data_interval = 5
    async with init_data_layer(
        wallet_rpc_port=wallet_rpc_port,
        bt=bt,
        db_path=tmp_path,
        manage_data_interval=manage_data_interval,
        maximum_full_file_count=100,
        group_files_by_store=group_files_by_store,
    ) as data_layer:
        data_rpc_api = DataLayerRpcApi(data_layer)
        res = await data_rpc_api.create_data_store({})
        root_hashes: list[bytes32] = []
        assert res is not None
        store_id = bytes32.from_hexstr(res["id"])
        await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)

        # subscribe to ourselves
        await data_rpc_api.subscribe(request={"id": store_id.hex()})
        update_count = 10
        for batch_count in range(update_count):
            key = batch_count.to_bytes(2, "big")
            value = batch_count.to_bytes(2, "big")
            changelist = [{"action": "insert", "key": key.hex(), "value": value.hex()}]
            res = await data_rpc_api.batch_update({"id": store_id.hex(), "changelist": changelist})
            update_tx_rec = res["tx_id"]
            await farm_block_with_spend(full_node_api, ph, update_tx_rec, wallet_rpc_api)
            await asyncio.sleep(manage_data_interval * 2)
            root_hash = await data_rpc_api.get_root({"id": store_id.hex()})
            root_hashes.append(root_hash["hash"])

        store_path = (
            data_layer.server_files_location.joinpath(f"{store_id}")
            if group_files_by_store
            else data_layer.server_files_location
        )
        filenames = {path.name for path in store_path.iterdir()}
        assert len(filenames) == 2 * update_count
        for generation, hash in enumerate(root_hashes):
            path = get_delta_filename_path(
                data_layer.server_files_location,
                store_id,
                hash,
                generation + 1,
                group_files_by_store,
            )
            assert path.exists()
            path = get_full_tree_filename_path(
                data_layer.server_files_location,
                store_id,
                hash,
                generation + 1,
                group_files_by_store,
            )
            assert path.exists()

        res = await data_rpc_api.unsubscribe(request={"id": store_id.hex(), "retain": retain})

        # wait for unsubscribe to be processed
        await asyncio.sleep(manage_data_interval * 3)

        filenames = {path.name for path in store_path.iterdir()}
        assert len(filenames) == (2 * update_count if retain else 0)


@pytest.mark.parametrize(argnames="layer", argvalues=list(InterfaceLayer))
@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_wallet_log_in_changes_active_fingerprint(
    self_hostname: str,
    one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices,
    layer: InterfaceLayer,
) -> None:
    wallet_rpc_api, _full_node_api, wallet_rpc_port, _ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )
    primary_fingerprint = cast(int, (await wallet_rpc_api.get_logged_in_fingerprint(request={}))["fingerprint"])

    mnemonic = create_mnemonic()
    assert wallet_rpc_api.service.local_keychain is not None
    private_key = wallet_rpc_api.service.local_keychain.add_key(mnemonic_or_pk=mnemonic)
    secondary_fingerprint: int = private_key.get_g1().get_fingerprint()

    await wallet_rpc_api.log_in(request={"fingerprint": primary_fingerprint})

    active_fingerprint = cast(int, (await wallet_rpc_api.get_logged_in_fingerprint(request={}))["fingerprint"])
    assert active_fingerprint == primary_fingerprint

    async with init_data_layer_service(wallet_rpc_port=wallet_rpc_port, bt=bt) as data_layer_service:
        # NOTE: we don't need the service for direct...  simpler to leave it in
        assert data_layer_service.rpc_server is not None
        rpc_port = data_layer_service.rpc_server.listen_port
        data_layer = data_layer_service._api.data_layer
        # test wallet log in
        data_rpc_api = DataLayerRpcApi(data_layer)

        if layer == InterfaceLayer.direct:
            await data_rpc_api.wallet_log_in({"fingerprint": secondary_fingerprint})
        elif layer == InterfaceLayer.client:
            async with DataLayerRpcClient.create_as_context(
                self_hostname=self_hostname,
                port=rpc_port,
                root_path=bt.root_path,
                net_config=bt.config,
            ) as client:
                await client.wallet_log_in(fingerprint=secondary_fingerprint)
        elif layer == InterfaceLayer.funcs:
            await wallet_log_in_cmd(rpc_port=rpc_port, fingerprint=secondary_fingerprint, root_path=bt.root_path)
        elif layer == InterfaceLayer.cli:
            process = await run_cli_cmd(
                "data",
                "wallet_log_in",
                "--fingerprint",
                str(secondary_fingerprint),
                "--data-rpc-port",
                str(rpc_port),
                root_path=bt.root_path,
            )
            assert process.stdout is not None
            assert await process.stdout.read() == b""
        else:  # pragma: no cover
            assert False, "unhandled parametrization"

        active_fingerprint = cast(int, (await wallet_rpc_api.get_logged_in_fingerprint(request={}))["fingerprint"])
        assert active_fingerprint == secondary_fingerprint


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_mirrors(
    self_hostname: str, one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices, tmp_path: Path
) -> None:
    wallet_rpc_api, full_node_api, wallet_rpc_port, ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )
    async with init_data_layer(wallet_rpc_port=wallet_rpc_port, bt=bt, db_path=tmp_path) as data_layer:
        data_rpc_api = DataLayerRpcApi(data_layer)
        res = await data_rpc_api.create_data_store({})
        assert res is not None
        store_id = bytes32.from_hexstr(res["id"])
        await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)

        urls = ["http://127.0.0.1/8000", "http://127.0.0.1/8001"]
        res = await data_rpc_api.add_mirror({"id": store_id.hex(), "urls": urls, "amount": 1, "fee": 1})

        await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)
        mirrors = await data_rpc_api.get_mirrors({"id": store_id.hex()})
        mirror_list = mirrors["mirrors"]
        assert len(mirror_list) == 1
        mirror = mirror_list[0]
        assert mirror["urls"] == ["http://127.0.0.1/8000", "http://127.0.0.1/8001"]
        coin_id = mirror["coin_id"]

        res = await data_rpc_api.delete_mirror({"coin_id": coin_id, "fee": 1})
        await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)
        mirrors = await data_rpc_api.get_mirrors({"id": store_id.hex()})
        mirror_list = mirrors["mirrors"]
        assert len(mirror_list) == 0

        with pytest.raises(RuntimeError, match="URL list can't be empty"):
            res = await data_rpc_api.add_mirror({"id": store_id.hex(), "urls": [], "amount": 1, "fee": 1})


@dataclass(frozen=True)
class ProofReference:
    entries_to_insert: int
    keys_to_prove: list[str]
    verify_proof_response: dict[str, Any]


def populate_reference(count: int, keys_to_prove: int) -> ProofReference:
    ret = ProofReference(
        entries_to_insert=count,
        keys_to_prove=[value.to_bytes(length=1, byteorder="big").hex() for value in range(keys_to_prove)],
        verify_proof_response={
            "current_root": True,
            "success": True,
            "verified_clvm_hashes": {
                "store_id": "",
                "inclusions": [
                    {
                        "key_clvm_hash": "0x" + std_hash(b"\1" + value.to_bytes(length=1, byteorder="big")).hex(),
                        "value_clvm_hash": "0x"
                        + std_hash(b"\1" + b"\x01" + value.to_bytes(length=1, byteorder="big")).hex(),
                    }
                    for value in range(keys_to_prove)
                ],
            },
        },
    )
    return ret


async def populate_proof_setup(offer_setup: OfferSetup, count: int) -> OfferSetup:
    if count > 0:
        # Only need data in the maker for proofs
        value_prefix = b"\x01"
        store_setup = offer_setup.maker
        await store_setup.api.batch_update(
            {
                "id": store_setup.id.hex(),
                "changelist": [
                    {
                        "action": "insert",
                        "key": value.to_bytes(length=1, byteorder="big").hex(),
                        "value": (value_prefix + value.to_bytes(length=1, byteorder="big")).hex(),
                    }
                    for value in range(count)
                ],
            }
        )

        await process_for_data_layer_keys(
            expected_key=b"\x00",
            full_node_api=offer_setup.full_node_api,
            data_layer=offer_setup.maker.data_layer,
            store_id=offer_setup.maker.id,
        )

    maker_original_singleton = await offer_setup.maker.data_layer.get_root(store_id=offer_setup.maker.id)
    assert maker_original_singleton is not None
    maker_original_root_hash = maker_original_singleton.root

    return OfferSetup(
        maker=StoreSetup(
            api=offer_setup.maker.api,
            id=offer_setup.maker.id,
            original_hash=maker_original_root_hash,
            data_layer=offer_setup.maker.data_layer,
            data_rpc_client=offer_setup.maker.data_rpc_client,
        ),
        taker=StoreSetup(
            api=offer_setup.taker.api,
            id=offer_setup.taker.id,
            original_hash=bytes32.zeros,
            data_layer=offer_setup.taker.data_layer,
            data_rpc_client=offer_setup.taker.data_rpc_client,
        ),
        full_node_api=offer_setup.full_node_api,
        wallet_nodes=offer_setup.wallet_nodes,
    )


@pytest.mark.parametrize(
    argnames="reference",
    argvalues=[
        pytest.param(populate_reference(count=5, keys_to_prove=1), id="one key"),
        pytest.param(populate_reference(count=5, keys_to_prove=2), id="two keys"),
        pytest.param(populate_reference(count=5, keys_to_prove=5), id="five keys"),
    ],
)
@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_dl_proof(offer_setup: OfferSetup, reference: ProofReference) -> None:
    offer_setup = await populate_proof_setup(offer_setup=offer_setup, count=reference.entries_to_insert)
    reference.verify_proof_response["verified_clvm_hashes"]["store_id"] = f"0x{offer_setup.maker.id.hex()}"

    #
    # Ideally this would use the InterfaceLayer as a parameterized list, however, all the fixtures
    # are function scoped, which makes it very long to run this but this doesn't change any of the
    # data, so rerunning all the setup for each test is not needed - module scope would be perfect
    # but it requires all the supporting fixtures (wallet/nodes/etc) to have the same scope
    #

    # random tests for HashOnlyProof root()
    fakeproof = HashOnlyProof(
        key_clvm_hash=bytes32([1] * 32), value_clvm_hash=bytes32([1] * 32), node_hash=bytes32([3] * 32), layers=[]
    )
    assert fakeproof.root() == fakeproof.node_hash

    fakeproof = HashOnlyProof(
        key_clvm_hash=bytes32([1] * 32),
        value_clvm_hash=bytes32([1] * 32),
        node_hash=bytes32([3] * 32),
        layers=[
            ProofLayer(other_hash_side=uint8(0), other_hash=bytes32([1] * 32), combined_hash=bytes32([5] * 32)),
            ProofLayer(other_hash_side=uint8(0), other_hash=bytes32([1] * 32), combined_hash=bytes32([7] * 32)),
        ],
    )
    assert fakeproof.root() == bytes32([7] * 32)

    # Test InterfaceLayer.direct
    proof = await offer_setup.maker.api.get_proof(
        request={"store_id": offer_setup.maker.id.hex(), "keys": reference.keys_to_prove}
    )
    assert proof["success"] is True
    verify = await offer_setup.taker.api.verify_proof(request=proof["proof"])
    assert verify == reference.verify_proof_response

    # test InterfaceLayer.client
    proof = dict()
    verify = dict()
    proof = await offer_setup.maker.data_rpc_client.get_proof(
        store_id=offer_setup.maker.id, keys=[hexstr_to_bytes(key) for key in reference.keys_to_prove]
    )
    assert proof["success"] is True
    verify = await offer_setup.taker.data_rpc_client.verify_proof(proof=proof["proof"])
    assert verify == reference.verify_proof_response

    # test InterfaceLayer.func
    proof = dict()
    verify = dict()
    proof = await get_proof_cmd(
        store_id=offer_setup.maker.id,
        key_strings=reference.keys_to_prove,
        rpc_port=offer_setup.maker.data_rpc_client.port,
        root_path=offer_setup.maker.data_layer.root_path,
    )
    assert proof["success"] is True
    verify = await verify_proof_cmd(
        proof=proof["proof"],
        rpc_port=offer_setup.taker.data_rpc_client.port,
        root_path=offer_setup.taker.data_layer.root_path,
    )
    assert verify == reference.verify_proof_response

    # test InterfaceLayer.cli
    key_args: list[str] = []
    for key in reference.keys_to_prove:
        key_args.append("--key")
        key_args.append(key)

    process = await run_cli_cmd(
        "data",
        "get_proof",
        "--id",
        offer_setup.maker.id.hex(),
        *key_args,
        "--data-rpc-port",
        str(offer_setup.maker.data_rpc_client.port),
        root_path=offer_setup.maker.data_layer.root_path,
    )
    assert process.stdout is not None
    raw_output = await process.stdout.read()
    proof = json.loads(raw_output)
    assert proof["success"] is True

    process = await run_cli_cmd(
        "data",
        "verify_proof",
        "-p",
        json.dumps(proof["proof"]),
        "--data-rpc-port",
        str(offer_setup.taker.data_rpc_client.port),
        root_path=offer_setup.taker.data_layer.root_path,
    )
    assert process.stdout is not None
    raw_output = await process.stdout.read()
    verify = json.loads(raw_output)
    assert verify == reference.verify_proof_response


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_dl_proof_errors(
    self_hostname: str, one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices, tmp_path: Path
) -> None:
    wallet_rpc_api, full_node_api, wallet_rpc_port, ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )
    async with init_data_layer(wallet_rpc_port=wallet_rpc_port, bt=bt, db_path=tmp_path) as data_layer:
        data_rpc_api = DataLayerRpcApi(data_layer)
        fakeroot = bytes32([4] * 32)
        res = await data_rpc_api.create_data_store({})
        assert res is not None
        store_id = bytes32.from_hexstr(res["id"])
        await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)

        with pytest.raises(ValueError, match="no root"):
            await data_rpc_api.get_proof(request={"store_id": fakeroot.hex(), "keys": []})

        with pytest.raises(Exception, match="No generations found"):
            await data_rpc_api.get_proof(request={"store_id": store_id.hex(), "keys": [b"4".hex()]})

        changelist: list[dict[str, str]] = [{"action": "insert", "key": b"a".hex(), "value": b"\x00\x01".hex()}]
        res = await data_rpc_api.batch_update({"id": store_id.hex(), "changelist": changelist})
        update_tx_rec0 = res["tx_id"]
        await farm_block_with_spend(full_node_api, ph, update_tx_rec0, wallet_rpc_api)

        with pytest.raises(KeyNotFoundError, match="Key not found"):
            await data_rpc_api.get_proof(request={"store_id": store_id.hex(), "keys": [b"4".hex()]})


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_dl_proof_verify_errors(offer_setup: OfferSetup, seeded_random: random.Random) -> None:
    two_key_proof = populate_reference(count=5, keys_to_prove=2)
    offer_setup = await populate_proof_setup(offer_setup=offer_setup, count=two_key_proof.entries_to_insert)
    two_key_proof.verify_proof_response["verified_clvm_hashes"]["store_id"] = f"0x{offer_setup.maker.id.hex()}"

    proof = await offer_setup.maker.api.get_proof(
        request={"store_id": offer_setup.maker.id.hex(), "keys": two_key_proof.keys_to_prove}
    )
    assert proof["success"] is True

    verify = await offer_setup.taker.api.verify_proof(request=proof["proof"])
    assert verify == two_key_proof.verify_proof_response

    # test bad coin id
    badproof = deepcopy(proof["proof"])
    badproof["coin_id"] = bytes32.random(seeded_random).hex()
    with pytest.raises(ValueError, match="Invalid Proof: No DL singleton found at coin id"):
        await offer_setup.taker.api.verify_proof(request=badproof)

    # test bad innerpuz
    badproof = deepcopy(proof["proof"])
    badproof["inner_puzzle_hash"] = bytes32.random(seeded_random).hex()
    with pytest.raises(ValueError, match="Invalid Proof: incorrect puzzle hash"):
        await offer_setup.taker.api.verify_proof(request=badproof)

    # test bad key
    badproof = deepcopy(proof["proof"])
    badproof["store_proofs"]["proofs"][0]["key_clvm_hash"] = bytes32.random(seeded_random).hex()
    with pytest.raises(ValueError, match="Invalid Proof: node hash does not match key and value"):
        await offer_setup.taker.api.verify_proof(request=badproof)

    # test bad value
    badproof = deepcopy(proof["proof"])
    badproof["store_proofs"]["proofs"][0]["value_clvm_hash"] = bytes32.random(seeded_random).hex()
    with pytest.raises(ValueError, match="Invalid Proof: node hash does not match key and value"):
        await offer_setup.taker.api.verify_proof(request=badproof)

    # test bad layer hash
    badproof = deepcopy(proof["proof"])
    badproof["store_proofs"]["proofs"][0]["layers"][1]["other_hash"] = bytes32.random(seeded_random).hex()
    with pytest.raises(ValueError, match="Invalid Proof: invalid proof of inclusion found"):
        await offer_setup.taker.api.verify_proof(request=badproof)


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_dl_proof_changed_root(offer_setup: OfferSetup, seeded_random: random.Random) -> None:
    two_key_proof = populate_reference(count=5, keys_to_prove=2)
    offer_setup = await populate_proof_setup(offer_setup=offer_setup, count=two_key_proof.entries_to_insert)
    two_key_proof.verify_proof_response["verified_clvm_hashes"]["store_id"] = f"0x{offer_setup.maker.id.hex()}"

    proof = await offer_setup.maker.api.get_proof(
        request={"store_id": offer_setup.maker.id.hex(), "keys": two_key_proof.keys_to_prove}
    )
    assert proof["success"] is True

    verify = await offer_setup.taker.api.verify_proof(request=proof["proof"])
    assert verify == two_key_proof.verify_proof_response

    key = b"a"
    value = b"\x00\x01"
    changelist: list[dict[str, str]] = [{"action": "insert", "key": key.hex(), "value": value.hex()}]
    await offer_setup.maker.api.batch_update({"id": offer_setup.maker.id.hex(), "changelist": changelist})

    await process_for_data_layer_keys(
        expected_key=key,
        expected_value=value,
        full_node_api=offer_setup.full_node_api,
        data_layer=offer_setup.maker.data_layer,
        store_id=offer_setup.maker.id,
    )

    root_changed = await offer_setup.taker.api.verify_proof(request=proof["proof"])
    assert root_changed == {**verify, "current_root": False}


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.anyio
async def test_pagination_rpcs(
    self_hostname: str, one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices, tmp_path: Path
) -> None:
    wallet_rpc_api, full_node_api, wallet_rpc_port, ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )
    # TODO: with this being a pseudo context manager'ish thing it doesn't actually handle shutdown
    async with init_data_layer(wallet_rpc_port=wallet_rpc_port, bt=bt, db_path=tmp_path) as data_layer:
        data_rpc_api = DataLayerRpcApi(data_layer)
        res = await data_rpc_api.create_data_store({})
        assert res is not None
        store_id = bytes32.from_hexstr(res["id"])
        await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)
        key1 = b"aa"
        value1 = b"\x01\x02"
        key1_hash = key_hash(key1)
        leaf_hash1 = leaf_hash(key1, value1)
        changelist: list[dict[str, str]] = [{"action": "insert", "key": key1.hex(), "value": value1.hex()}]
        key2 = b"ba"
        value2 = b"\x03\x02"
        key2_hash = key_hash(key2)
        leaf_hash2 = leaf_hash(key2, value2)
        changelist.append({"action": "insert", "key": key2.hex(), "value": value2.hex()})
        key3 = b"ccc"
        value3 = b"\x04\x05"
        changelist.append({"action": "insert", "key": key3.hex(), "value": value3.hex()})
        leaf_hash3 = leaf_hash(key3, value3)
        key4 = b"d"
        value4 = b"\x06\x03"
        key4_hash = key_hash(key4)
        leaf_hash4 = leaf_hash(key4, value4)
        changelist.append({"action": "insert", "key": key4.hex(), "value": value4.hex()})
        key5 = b"e"
        value5 = b"\x07\x01"
        key5_hash = key_hash(key5)
        leaf_hash5 = leaf_hash(key5, value5)
        changelist.append({"action": "insert", "key": key5.hex(), "value": value5.hex()})
        res = await data_rpc_api.batch_update({"id": store_id.hex(), "changelist": changelist})
        update_tx_rec0 = res["tx_id"]
        await farm_block_with_spend(full_node_api, ph, update_tx_rec0, wallet_rpc_api)
        local_root = await data_rpc_api.get_local_root({"id": store_id.hex()})

        keys_reference = {
            "total_pages": 2,
            "total_bytes": 9,
            "keys": [],
            "root_hash": local_root["hash"],
        }

        keys_paginated = await data_rpc_api.get_keys({"id": store_id.hex(), "page": 0, "max_page_size": 5})
        assert key2_hash < key1_hash
        assert keys_paginated == {**keys_reference, "keys": ["0x" + key3.hex(), "0x" + key2.hex()]}

        keys_paginated = await data_rpc_api.get_keys({"id": store_id.hex(), "page": 1, "max_page_size": 5})
        assert key5_hash < key4_hash
        assert keys_paginated == {**keys_reference, "keys": ["0x" + key1.hex(), "0x" + key5.hex(), "0x" + key4.hex()]}

        keys_paginated = await data_rpc_api.get_keys({"id": store_id.hex(), "page": 2, "max_page_size": 5})
        assert keys_paginated == keys_reference

        keys_values_reference = {
            "total_pages": 3,
            "total_bytes": 19,
            "keys_values": [],
            "root_hash": local_root["hash"],
        }
        keys_values_paginated = await data_rpc_api.get_keys_values(
            {"id": store_id.hex(), "page": 0, "max_page_size": 8},
        )
        expected_kv = [
            {"atom": None, "hash": "0x" + leaf_hash3.hex(), "key": "0x" + key3.hex(), "value": "0x" + value3.hex()},
        ]
        assert keys_values_paginated == {**keys_values_reference, "keys_values": expected_kv}

        keys_values_paginated = await data_rpc_api.get_keys_values(
            {"id": store_id.hex(), "page": 1, "max_page_size": 8}
        )
        expected_kv = [
            {"atom": None, "hash": "0x" + leaf_hash1.hex(), "key": "0x" + key1.hex(), "value": "0x" + value1.hex()},
            {"atom": None, "hash": "0x" + leaf_hash2.hex(), "key": "0x" + key2.hex(), "value": "0x" + value2.hex()},
        ]
        assert leaf_hash1 < leaf_hash2
        assert keys_values_paginated == {**keys_values_reference, "keys_values": expected_kv}

        keys_values_paginated = await data_rpc_api.get_keys_values(
            {"id": store_id.hex(), "page": 2, "max_page_size": 8}
        )
        expected_kv = [
            {"atom": None, "hash": "0x" + leaf_hash5.hex(), "key": "0x" + key5.hex(), "value": "0x" + value5.hex()},
            {"atom": None, "hash": "0x" + leaf_hash4.hex(), "key": "0x" + key4.hex(), "value": "0x" + value4.hex()},
        ]
        assert leaf_hash5 < leaf_hash4
        assert keys_values_paginated == {**keys_values_reference, "keys_values": expected_kv}

        keys_values_paginated = await data_rpc_api.get_keys_values(
            {"id": store_id.hex(), "page": 3, "max_page_size": 8}
        )
        assert keys_values_paginated == keys_values_reference

        key6 = b"ab"
        value6 = b"\x01\x01"
        leaf_hash6 = leaf_hash(key6, value6)
        key7 = b"ac"
        value7 = b"\x01\x01"
        leaf_hash7 = leaf_hash(key7, value7)

        changelist = [{"action": "delete", "key": key3.hex()}]
        changelist.append({"action": "insert", "key": key6.hex(), "value": value6.hex()})
        changelist.append({"action": "insert", "key": key7.hex(), "value": value7.hex()})

        res = await data_rpc_api.batch_update({"id": store_id.hex(), "changelist": changelist})
        update_tx_rec1 = res["tx_id"]
        await farm_block_with_spend(full_node_api, ph, update_tx_rec1, wallet_rpc_api)

        history = await data_rpc_api.get_root_history({"id": store_id.hex()})
        hash1 = history["root_history"][1]["root_hash"]
        hash2 = history["root_history"][2]["root_hash"]
        diff_reference = {
            "total_pages": 3,
            "total_bytes": 13,
            "diff": [],
        }
        diff_res = await data_rpc_api.get_kv_diff(
            {
                "id": store_id.hex(),
                "hash_1": hash1.hex(),
                "hash_2": hash2.hex(),
                "page": 0,
                "max_page_size": 5,
            }
        )
        expected_diff = [{"type": "DELETE", "key": key3.hex(), "value": value3.hex()}]
        assert diff_res == {**diff_reference, "diff": expected_diff}

        diff_res = await data_rpc_api.get_kv_diff(
            {
                "id": store_id.hex(),
                "hash_1": hash1.hex(),
                "hash_2": hash2.hex(),
                "page": 1,
                "max_page_size": 5,
            }
        )
        assert leaf_hash6 < leaf_hash7
        expected_diff = [{"type": "INSERT", "key": key6.hex(), "value": value6.hex()}]
        assert diff_res == {**diff_reference, "diff": expected_diff}

        diff_res = await data_rpc_api.get_kv_diff(
            {
                "id": store_id.hex(),
                "hash_1": hash1.hex(),
                "hash_2": hash2.hex(),
                "page": 2,
                "max_page_size": 5,
            }
        )
        expected_diff = [{"type": "INSERT", "key": key7.hex(), "value": value7.hex()}]
        assert diff_res == {**diff_reference, "diff": expected_diff}

        diff_res = await data_rpc_api.get_kv_diff(
            {
                "id": store_id.hex(),
                "hash_1": hash1.hex(),
                "hash_2": hash2.hex(),
                "page": 3,
                "max_page_size": 5,
            }
        )
        assert diff_res == diff_reference

        invalid_hash = bytes32([0] * 31 + [1])
        with pytest.raises(Exception, match=f"Unable to diff: Can't find keys and values for {invalid_hash.hex()}"):
            await data_rpc_api.get_kv_diff(
                {
                    "id": store_id.hex(),
                    "hash_1": hash1.hex(),
                    "hash_2": invalid_hash.hex(),
                    "page": 0,
                    "max_page_size": 10,
                }
            )

        with pytest.raises(Exception, match=f"Unable to diff: Can't find keys and values for {invalid_hash.hex()}"):
            diff_res = await data_rpc_api.get_kv_diff(
                {
                    "id": store_id.hex(),
                    "hash_1": invalid_hash.hex(),
                    "hash_2": hash2.hex(),
                    "page": 0,
                    "max_page_size": 10,
                }
            )

        new_value = b"\x02\x02"
        changelist = [{"action": "upsert", "key": key6.hex(), "value": new_value.hex()}]
        new_leaf_hash = leaf_hash(key6, new_value)
        res = await data_rpc_api.batch_update({"id": store_id.hex(), "changelist": changelist})
        update_tx_rec3 = res["tx_id"]
        await farm_block_with_spend(full_node_api, ph, update_tx_rec3, wallet_rpc_api)

        history = await data_rpc_api.get_root_history({"id": store_id.hex()})
        hash1 = history["root_history"][2]["root_hash"]
        hash2 = history["root_history"][3]["root_hash"]

        diff_res = await data_rpc_api.get_kv_diff(
            {
                "id": store_id.hex(),
                "hash_1": hash1.hex(),
                "hash_2": hash2.hex(),
                "page": 0,
                "max_page_size": 100,
            }
        )
        assert leaf_hash6 < new_leaf_hash
        diff_reference = {
            "total_pages": 1,
            "total_bytes": 8,
            "diff": [
                {"type": "INSERT", "key": key6.hex(), "value": new_value.hex()},
                {"type": "DELETE", "key": key6.hex(), "value": value6.hex()},
            ],
        }
        assert diff_res == diff_reference

        with pytest.raises(Exception, match="Cannot find merkle blob"):
            await data_rpc_api.get_keys(
                {"id": store_id.hex(), "page": 0, "max_page_size": 100, "root_hash": bytes32([0] * 31 + [1]).hex()}
            )

        with pytest.raises(Exception, match="Cannot find merkle blob"):
            await data_rpc_api.get_keys_values(
                {"id": store_id.hex(), "page": 0, "max_page_size": 100, "root_hash": bytes32([0] * 31 + [1]).hex()}
            )

        with pytest.raises(RuntimeError, match="Cannot paginate data, item size is larger than max page size"):
            keys_paginated = await data_rpc_api.get_keys_values({"id": store_id.hex(), "page": 0, "max_page_size": 1})

        with pytest.raises(RuntimeError, match="Cannot paginate data, item size is larger than max page size"):
            keys_values_paginated = await data_rpc_api.get_keys_values(
                {"id": store_id.hex(), "page": 0, "max_page_size": 1}
            )

        with pytest.raises(RuntimeError, match="Cannot paginate data, item size is larger than max page size"):
            diff_res = await data_rpc_api.get_kv_diff(
                {
                    "id": store_id.hex(),
                    "hash_1": hash1.hex(),
                    "hash_2": hash2.hex(),
                    "page": 0,
                    "max_page_size": 1,
                }
            )


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.parametrize(argnames="layer", argvalues=[InterfaceLayer.funcs, InterfaceLayer.cli, InterfaceLayer.client])
@pytest.mark.parametrize(argnames="max_page_size", argvalues=[5, 100, None])
@pytest.mark.anyio
async def test_pagination_cmds(
    self_hostname: str,
    one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices,
    tmp_path: Path,
    layer: InterfaceLayer,
    max_page_size: Optional[int],
    bt: BlockTools,
) -> None:
    wallet_rpc_api, full_node_api, wallet_rpc_port, ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )
    async with init_data_layer_service(
        wallet_rpc_port=wallet_rpc_port,
        bt=bt,
        db_path=tmp_path,
        enable_batch_autoinsert=False,
    ) as data_layer_service:
        assert data_layer_service.rpc_server is not None
        rpc_port = data_layer_service.rpc_server.listen_port
        data_layer = data_layer_service._api.data_layer
        data_rpc_api = DataLayerRpcApi(data_layer)

        res = await data_rpc_api.create_data_store({})
        assert res is not None
        store_id = bytes32.from_hexstr(res["id"])
        await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)

        key = b"aa"
        value = b"aa"
        key_2 = b"aaaa"
        value_2 = b"a"

        changelist = [
            {"action": "insert", "key": key.hex(), "value": value.hex()},
            {"action": "insert", "key": key_2.hex(), "value": value_2.hex()},
        ]

        res = await data_rpc_api.batch_update({"id": store_id.hex(), "changelist": changelist})
        update_tx_rec0 = res["tx_id"]
        await farm_block_with_spend(full_node_api, ph, update_tx_rec0, wallet_rpc_api)
        local_root = await data_rpc_api.get_local_root({"id": store_id.hex()})
        hash_1 = bytes32.zeros
        hash_2 = local_root["hash"]
        # `InterfaceLayer.direct` is not tested here since test `test_pagination_rpcs` extensively use it.
        if layer == InterfaceLayer.funcs:
            keys = await get_keys_cmd(
                rpc_port=rpc_port,
                store_id=store_id,
                root_hash=None,
                fingerprint=None,
                page=0,
                max_page_size=max_page_size,
                root_path=bt.root_path,
            )
            keys_values = await get_keys_values_cmd(
                rpc_port=rpc_port,
                store_id=store_id,
                root_hash=None,
                fingerprint=None,
                page=0,
                max_page_size=max_page_size,
                root_path=bt.root_path,
            )
            kv_diff = await get_kv_diff_cmd(
                rpc_port=rpc_port,
                store_id=store_id,
                hash_1=hash_1,
                hash_2=hash_2,
                fingerprint=None,
                page=0,
                max_page_size=max_page_size,
                root_path=bt.root_path,
            )
        elif layer == InterfaceLayer.cli:
            for command in ("get_keys", "get_keys_values", "get_kv_diff"):
                if command in {"get_keys", "get_keys_values"}:
                    args: list[str] = [
                        sys.executable,
                        "-m",
                        "chia",
                        "data",
                        command,
                        "--id",
                        store_id.hex(),
                        "--data-rpc-port",
                        str(rpc_port),
                        "--page",
                        "0",
                    ]
                else:
                    args = [
                        sys.executable,
                        "-m",
                        "chia",
                        "data",
                        command,
                        "--id",
                        store_id.hex(),
                        "--hash_1",
                        "0x" + hash_1.hex(),
                        "--hash_2",
                        "0x" + hash_2.hex(),
                        "--data-rpc-port",
                        str(rpc_port),
                        "--page",
                        "0",
                    ]
                if max_page_size is not None:
                    args.append("--max-page-size")
                    args.append(f"{max_page_size}")
                process = await asyncio.create_subprocess_exec(
                    *args,
                    env={**os.environ, "CHIA_ROOT": str(bt.root_path)},
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                await process.wait()
                assert process.stdout is not None
                assert process.stderr is not None
                stdout = await process.stdout.read()
                stderr = await process.stderr.read()
                if command == "get_keys":
                    keys = json.loads(stdout)
                elif command == "get_keys_values":
                    keys_values = json.loads(stdout)
                else:
                    kv_diff = json.loads(stdout)
                assert process.returncode == 0
                if sys.version_info >= (3, 10, 6):
                    assert stderr == b""
                else:  # pragma: no cover
                    # https://github.com/python/cpython/issues/92841
                    assert stderr == b"" or b"_ProactorBasePipeTransport.__del__" in stderr
        elif layer == InterfaceLayer.client:
            async with DataLayerRpcClient.create_as_context(
                self_hostname=self_hostname,
                port=rpc_port,
                root_path=bt.root_path,
                net_config=bt.config,
            ) as client:
                keys = await client.get_keys(
                    store_id=store_id,
                    root_hash=None,
                    page=0,
                    max_page_size=max_page_size,
                )
                keys_values = await client.get_keys_values(
                    store_id=store_id,
                    root_hash=None,
                    page=0,
                    max_page_size=max_page_size,
                )
                kv_diff = await client.get_kv_diff(
                    store_id=store_id,
                    hash_1=hash_1,
                    hash_2=hash_2,
                    page=0,
                    max_page_size=max_page_size,
                )
        else:  # pragma: no cover
            assert False, "unhandled parametrization"
        if max_page_size is None or max_page_size == 100:
            assert keys == {
                "keys": ["0x61616161", "0x6161"],
                "root_hash": "0x3f4ae7b8e10ef48b3114843537d5def989ee0a3b6568af7e720a71730f260fa1",
                "success": True,
                "total_bytes": 6,
                "total_pages": 1,
            }
            assert keys_values == {
                "keys_values": [
                    {
                        "atom": None,
                        "hash": "0x3c8ecfd41a1c54820f5ad687a4cbfbad0faa78445cbf31ec4f879ce553216a9d",
                        "key": "0x61616161",
                        "value": "0x61",
                    },
                    {
                        "atom": None,
                        "hash": "0x5a7edd8e4bc28e32ba2a2514054f3872037a4f6da52c5a662969b6b881beaa3f",
                        "key": "0x6161",
                        "value": "0x6161",
                    },
                ],
                "root_hash": "0x3f4ae7b8e10ef48b3114843537d5def989ee0a3b6568af7e720a71730f260fa1",
                "success": True,
                "total_bytes": 9,
                "total_pages": 1,
            }
            assert kv_diff == {
                "diff": [
                    {"key": "61616161", "type": "INSERT", "value": "61"},
                    {"key": "6161", "type": "INSERT", "value": "6161"},
                ],
                "success": True,
                "total_bytes": 9,
                "total_pages": 1,
            }
        elif max_page_size == 5:
            assert keys == {
                "keys": ["0x61616161"],
                "root_hash": "0x3f4ae7b8e10ef48b3114843537d5def989ee0a3b6568af7e720a71730f260fa1",
                "success": True,
                "total_bytes": 6,
                "total_pages": 2,
            }
            assert keys_values == {
                "keys_values": [
                    {
                        "atom": None,
                        "hash": "0x3c8ecfd41a1c54820f5ad687a4cbfbad0faa78445cbf31ec4f879ce553216a9d",
                        "key": "0x61616161",
                        "value": "0x61",
                    }
                ],
                "root_hash": "0x3f4ae7b8e10ef48b3114843537d5def989ee0a3b6568af7e720a71730f260fa1",
                "success": True,
                "total_bytes": 9,
                "total_pages": 2,
            }
            assert kv_diff == {
                "diff": [
                    {"key": "61616161", "type": "INSERT", "value": "61"},
                ],
                "success": True,
                "total_bytes": 9,
                "total_pages": 2,
            }
        else:  # pragma: no cover
            assert False, "unhandled parametrization"


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.parametrize(argnames="layer", argvalues=list(InterfaceLayer))
@pytest.mark.anyio
async def test_unsubmitted_batch_update(
    self_hostname: str,
    one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices,
    tmp_path: Path,
    layer: InterfaceLayer,
    bt: BlockTools,
) -> None:
    wallet_rpc_api, full_node_api, wallet_rpc_port, ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )
    # Number of farmed blocks to check our batch update was not submitted.
    NUM_BLOCKS_WITHOUT_SUBMIT = 10
    async with init_data_layer_service(wallet_rpc_port=wallet_rpc_port, bt=bt, db_path=tmp_path) as data_layer_service:
        assert data_layer_service.rpc_server is not None
        rpc_port = data_layer_service.rpc_server.listen_port
        data_layer = data_layer_service._api.data_layer
        data_rpc_api = DataLayerRpcApi(data_layer)

        res = await data_rpc_api.create_data_store({})
        assert res is not None

        store_id = bytes32.from_hexstr(res["id"])
        await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)

        to_insert = [(b"a", b"\x00\x01"), (b"b", b"\x00\x02"), (b"c", b"\x00\x03")]
        for key, value in to_insert:
            changelist: list[dict[str, str]] = [{"action": "insert", "key": key.hex(), "value": value.hex()}]

            if layer == InterfaceLayer.direct:
                res = await data_rpc_api.batch_update(
                    {"id": store_id.hex(), "changelist": changelist, "submit_on_chain": False}
                )
                assert res == {}
            elif layer == InterfaceLayer.funcs:
                res = await update_data_store_cmd(
                    rpc_port=rpc_port,
                    store_id=store_id,
                    changelist=changelist,
                    fee=None,
                    fingerprint=None,
                    submit_on_chain=False,
                    root_path=bt.root_path,
                )
                assert res == {"success": True}
            elif layer == InterfaceLayer.cli:
                args: list[str] = [
                    sys.executable,
                    "-m",
                    "chia",
                    "data",
                    "update_data_store",
                    "--id",
                    store_id.hex(),
                    "--changelist",
                    json.dumps(changelist),
                    "--no-submit",
                    "--data-rpc-port",
                    str(rpc_port),
                ]
                process = await asyncio.create_subprocess_exec(
                    *args,
                    env={**os.environ, "CHIA_ROOT": str(bt.root_path)},
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                await process.wait()
                assert process.stdout is not None
                assert process.stderr is not None
                stdout = await process.stdout.read()
                res = json.loads(stdout)
                stderr = await process.stderr.read()
                assert process.returncode == 0
                if sys.version_info >= (3, 10, 6):
                    assert stderr == b""
                else:  # pragma: no cover
                    # https://github.com/python/cpython/issues/92841
                    assert stderr == b"" or b"_ProactorBasePipeTransport.__del__" in stderr
                assert res == {"success": True}
            elif layer == InterfaceLayer.client:
                async with DataLayerRpcClient.create_as_context(
                    self_hostname=self_hostname,
                    port=rpc_port,
                    root_path=bt.root_path,
                    net_config=bt.config,
                ) as client:
                    res = await client.update_data_store(
                        store_id=store_id,
                        changelist=changelist,
                        fee=None,
                        submit_on_chain=False,
                    )
                    assert res == {"success": True}
            else:  # pragma: no cover
                assert False, "unhandled parametrization"

            await full_node_api.farm_blocks_to_puzzlehash(
                count=NUM_BLOCKS_WITHOUT_SUBMIT, guarantee_transaction_blocks=True
            )
            keys_values = await data_rpc_api.get_keys_values({"id": store_id.hex()})
            assert keys_values == {"keys_values": []}
            pending_root = await data_layer.data_store.get_pending_root(store_id=store_id)
            assert pending_root is not None
            assert pending_root.status == Status.PENDING_BATCH

        key = b"d"
        value = b"\x00\x04"
        to_insert.append((key, value))

        changelist = [{"action": "insert", "key": key.hex(), "value": value.hex()}]
        res = await data_rpc_api.batch_update({"id": store_id.hex(), "changelist": changelist})
        update_tx_rec0 = res["tx_id"]
        await farm_block_with_spend(full_node_api, ph, update_tx_rec0, wallet_rpc_api)

        keys_values = await data_rpc_api.get_keys_values({"id": store_id.hex()})
        assert len(keys_values["keys_values"]) == len(to_insert)
        kv_dict = {item["key"]: item["value"] for item in keys_values["keys_values"]}
        for key, value in to_insert:
            assert kv_dict["0x" + key.hex()] == "0x" + value.hex()
        prev_keys_values = keys_values
        old_root = await data_layer.data_store.get_tree_root(store_id=store_id)

        key = b"e"
        value = b"\x00\x05"
        changelist = [{"action": "insert", "key": key.hex(), "value": value.hex()}]
        res = await data_rpc_api.batch_update(
            {"id": store_id.hex(), "changelist": changelist, "submit_on_chain": False}
        )
        assert res == {}

        await full_node_api.farm_blocks_to_puzzlehash(
            count=NUM_BLOCKS_WITHOUT_SUBMIT, guarantee_transaction_blocks=True
        )
        root = await data_layer.data_store.get_tree_root(store_id=store_id)
        assert root == old_root

        key = b"f"
        value = b"\x00\x06"
        changelist = [{"action": "insert", "key": key.hex(), "value": value.hex()}]
        res = await data_rpc_api.batch_update(
            {"id": store_id.hex(), "changelist": changelist, "submit_on_chain": False}
        )
        assert res == {}

        await full_node_api.farm_blocks_to_puzzlehash(
            count=NUM_BLOCKS_WITHOUT_SUBMIT, guarantee_transaction_blocks=True
        )

        await data_rpc_api.clear_pending_roots({"store_id": store_id.hex()})
        pending_root = await data_layer.data_store.get_pending_root(store_id=store_id)
        assert pending_root is None
        root = await data_layer.data_store.get_tree_root(store_id=store_id)
        assert root == old_root

        key = b"g"
        value = b"\x00\x07"
        changelist = [{"action": "insert", "key": key.hex(), "value": value.hex()}]
        to_insert.append((key, value))

        res = await data_rpc_api.batch_update(
            {"id": store_id.hex(), "changelist": changelist, "submit_on_chain": False}
        )
        assert res == {}

        await full_node_api.farm_blocks_to_puzzlehash(
            count=NUM_BLOCKS_WITHOUT_SUBMIT, guarantee_transaction_blocks=True
        )
        keys_values = await data_rpc_api.get_keys_values({"id": store_id.hex()})
        # order agnostic comparison of the list of dicts
        assert {item["key"]: item for item in keys_values["keys_values"]} == {
            item["key"]: item for item in prev_keys_values["keys_values"]
        }

        pending_root = await data_layer.data_store.get_pending_root(store_id=store_id)
        assert pending_root is not None
        assert pending_root.status == Status.PENDING_BATCH

        # submit pending root
        if layer == InterfaceLayer.direct:
            res = await data_rpc_api.submit_pending_root({"id": store_id.hex()})
            update_tx_rec1 = res["tx_id"]
        elif layer == InterfaceLayer.funcs:
            res = await submit_pending_root_cmd(
                store_id=store_id,
                fee=None,
                fingerprint=None,
                rpc_port=rpc_port,
                root_path=bt.root_path,
            )
            update_tx_rec1 = bytes32.from_hexstr(res["tx_id"])
        elif layer == InterfaceLayer.cli:
            args = [
                sys.executable,
                "-m",
                "chia",
                "data",
                "submit_pending_root",
                "--id",
                store_id.hex(),
                "--data-rpc-port",
                str(rpc_port),
            ]
            process = await asyncio.create_subprocess_exec(
                *args,
                env={**os.environ, "CHIA_ROOT": str(bt.root_path)},
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            await process.wait()
            assert process.stdout is not None
            assert process.stderr is not None
            stdout = await process.stdout.read()
            res = json.loads(stdout)
            stderr = await process.stderr.read()
            assert process.returncode == 0
            if sys.version_info >= (3, 10, 6):
                assert stderr == b""
            else:  # pragma: no cover
                # https://github.com/python/cpython/issues/92841
                assert stderr == b"" or b"_ProactorBasePipeTransport.__del__" in stderr
            update_tx_rec1 = bytes32.from_hexstr(res["tx_id"])
        elif layer == InterfaceLayer.client:
            async with DataLayerRpcClient.create_as_context(
                self_hostname=self_hostname,
                port=rpc_port,
                root_path=bt.root_path,
                net_config=bt.config,
            ) as client:
                res = await client.submit_pending_root(store_id=store_id, fee=None)
                update_tx_rec1 = bytes32.from_hexstr(res["tx_id"])
        else:  # pragma: no cover
            assert False, "unhandled parametrization"

        pending_root = await data_layer.data_store.get_pending_root(store_id=store_id)
        assert pending_root is not None
        assert pending_root.status == Status.PENDING

        key = b"h"
        value = b"\x00\x08"
        changelist = [{"action": "insert", "key": key.hex(), "value": value.hex()}]
        with pytest.raises(Exception, match="Already have a pending root waiting for confirmation"):
            res = await data_rpc_api.batch_update(
                {"id": store_id.hex(), "changelist": changelist, "submit_on_chain": False}
            )
        with pytest.raises(Exception, match="Pending root is already submitted"):
            res = await data_rpc_api.submit_pending_root({"id": store_id.hex()})

        await farm_block_with_spend(full_node_api, ph, update_tx_rec1, wallet_rpc_api)

        keys_values = await data_rpc_api.get_keys_values({"id": store_id.hex()})
        assert len(keys_values["keys_values"]) == len(to_insert)
        kv_dict = {item["key"]: item["value"] for item in keys_values["keys_values"]}
        for key, value in to_insert:
            assert kv_dict["0x" + key.hex()] == "0x" + value.hex()

        with pytest.raises(Exception, match="Latest root is already confirmed"):
            res = await data_rpc_api.submit_pending_root({"id": store_id.hex()})


@pytest.mark.limit_consensus_modes(reason="does not depend on consensus rules")
@pytest.mark.parametrize(argnames="layer", argvalues=list(InterfaceLayer))
@boolean_datacases(name="submit_on_chain", false="save as incomplete batch", true="submit directly on chain")
@pytest.mark.anyio
async def test_multistore_update(
    self_hostname: str,
    one_wallet_and_one_simulator_services: SimulatorsAndWalletsServices,
    tmp_path: Path,
    layer: InterfaceLayer,
    submit_on_chain: bool,
) -> None:
    wallet_rpc_api, full_node_api, wallet_rpc_port, ph, bt = await init_wallet_and_node(
        self_hostname, one_wallet_and_one_simulator_services
    )
    async with init_data_layer_service(wallet_rpc_port=wallet_rpc_port, bt=bt, db_path=tmp_path) as data_layer_service:
        assert data_layer_service.rpc_server is not None
        rpc_port = data_layer_service.rpc_server.listen_port

        data_layer = data_layer_service._api.data_layer
        data_store = data_layer.data_store
        data_rpc_api = DataLayerRpcApi(data_layer)

        store_ids: list[bytes32] = []
        store_ids_count = 5

        for _ in range(store_ids_count):
            res = await data_rpc_api.create_data_store({})
            assert res is not None
            store_id = bytes32.from_hexstr(res["id"])
            await farm_block_check_singleton(data_layer, full_node_api, ph, store_id, wallet=wallet_rpc_api.service)
            store_ids.append(store_id)

        store_updates: list[dict[str, Any]] = []
        key_offset = 1000
        for index, store_id in enumerate(store_ids):
            changelist: list[dict[str, str]] = []
            key = index.to_bytes(2, "big")
            value = index.to_bytes(2, "big")
            changelist.append({"action": "insert", "key": key.hex(), "value": value.hex()})
            key = (index + key_offset).to_bytes(2, "big")
            value = (index + key_offset).to_bytes(2, "big")
            changelist.append({"action": "insert", "key": key.hex(), "value": value.hex()})
            store_updates.append({"store_id": store_id.hex(), "changelist": changelist})

        if layer == InterfaceLayer.direct:
            res = await data_rpc_api.multistore_batch_update(
                {"store_updates": store_updates, "submit_on_chain": submit_on_chain}
            )
            if submit_on_chain:
                update_tx_rec0 = res["tx_id"][0]
            else:
                assert res == {}
        elif layer == InterfaceLayer.funcs:
            res = await update_multiple_stores_cmd(
                rpc_port=rpc_port,
                store_updates=store_updates,
                submit_on_chain=submit_on_chain,
                fee=None,
                fingerprint=None,
                root_path=bt.root_path,
            )
            if submit_on_chain:
                update_tx_rec0 = bytes32.from_hexstr(res["tx_id"][0])
            else:
                assert res == {"success": True}
        elif layer == InterfaceLayer.cli:
            process = await run_cli_cmd(
                "data",
                "update_multiple_stores",
                "--store_updates",
                json.dumps(store_updates),
                "--data-rpc-port",
                str(rpc_port),
                "--submit" if submit_on_chain else "--no-submit",
                root_path=bt.root_path,
            )
            assert process.stdout is not None
            raw_output = await process.stdout.read()
            res = json.loads(raw_output)

            if submit_on_chain:
                update_tx_rec0 = bytes32.from_hexstr(res["tx_id"][0])
            else:
                assert res == {"success": True}
        elif layer == InterfaceLayer.client:
            async with DataLayerRpcClient.create_as_context(
                self_hostname=self_hostname,
                port=rpc_port,
                root_path=bt.root_path,
                net_config=bt.config,
            ) as client:
                res = await client.update_multiple_stores(
                    store_updates=store_updates,
                    submit_on_chain=submit_on_chain,
                    fee=None,
                )

            if submit_o