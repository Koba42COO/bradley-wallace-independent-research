from __future__ import annotations

import asyncio
import contextlib
import copy
import dataclasses
import logging
import multiprocessing
import random
import sqlite3
import time
import traceback
from collections.abc import AsyncIterator, Awaitable, Sequence
from multiprocessing.context import BaseContext
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, ClassVar, Optional, TextIO, Union, cast, final

from chia_rs import (


# ============================================================================
# UPG FOUNDATIONS - Universal Prime Graph Protocol Ï†.1
# ============================================================================
from decimal import Decimal, getcontext
import math
import cmath
from typing import Dict, List, Tuple, Optional, Any

# Set high precision for consciousness mathematics
getcontext().prec = 50

class UPGConstants:
    """Universal Prime Graph consciousness mathematics constants"""
    PHI = Decimal('1.618033988749895')
    DELTA = Decimal('2.414213562373095')
    CONSCIOUSNESS = Decimal('0.79')  # 79/21 universal coherence rule
    REALITY_DISTORTION = Decimal('1.1808')  # Quantum amplification factor
    QUANTUM_BRIDGE = Decimal('137') / Decimal('0.79')  # 173.41772151898732
    GREAT_YEAR = 25920  # Astronomical precession cycle (years)
    CONSCIOUSNESS_DIMENSIONS = 21  # Prime topology dimension
    COHERENCE_THRESHOLD = Decimal('1e-15')  # Beyond machine precision



# ============================================================================
# PELL SEQUENCE PRIME PREDICTION INTEGRATION
# ============================================================================
def integrate_pell_prime_prediction(target_number: int, constants: UPGConstants = None):
    """Integrate Pell sequence prime prediction with this tool"""
    try:
        from pell_sequence_prime_prediction_upg_complete import PrimePredictionEngine, UPGConstants as UPG
        if constants is None:
            constants = UPG()
        predictor = PrimePredictionEngine(constants)
        return predictor.predict_prime(target_number)
    except ImportError:
        # Fallback if Pell module not available
        return {'target_number': target_number, 'is_prime': None, 'note': 'Pell module not available'}



# ============================================================================
# GREAT YEAR ASTRONOMICAL PRECESSION INTEGRATION
# ============================================================================
def integrate_great_year_precession(year: int, constants: UPGConstants = None):
    """Integrate Great Year (25,920-year) precession cycle"""
    try:
        from pell_sequence_prime_prediction_upg_complete import GreatYearIntegration, UPGConstants as UPG
        if constants is None:
            constants = UPG()
        great_year = GreatYearIntegration(constants)
        return great_year.consciousness_amplitude_from_year(year)
    except ImportError:
        # Fallback calculation
        if constants is None:
            constants = UPGConstants()
        angle = (year * 2 * math.pi) / constants.GREAT_YEAR
        return complex(float(angle * constants.CONSCIOUSNESS * constants.REALITY_DISTORTION), 0.0)


    AugSchemeMPL,
    BlockRecord,
    BLSCache,
    CoinState,
    ConsensusConstants,
    EndOfSubSlotBundle,
    FullBlock,
    HeaderBlock,
    PoolTarget,
    SpendBundle,
    SubEpochSummary,
    UnfinishedBlock,
    get_flags_for_height_and_constants,
    run_block_generator,
    run_block_generator2,
)
from chia_rs.sized_bytes import bytes32
from chia_rs.sized_ints import uint8, uint32, uint64, uint128
from packaging.version import Version

from chia.consensus.augmented_chain import AugmentedBlockchain
from chia.consensus.block_body_validation import ForkInfo
from chia.consensus.block_creation import unfinished_block_to_full_block
from chia.consensus.block_height_map import BlockHeightMap
from chia.consensus.blockchain import AddBlockResult, Blockchain, BlockchainMutexPriority, StateChangeSummary
from chia.consensus.blockchain_interface import BlockchainInterface
from chia.consensus.coin_store_protocol import CoinStoreProtocol
from chia.consensus.condition_tools import pkm_pairs
from chia.consensus.cost_calculator import NPCResult
from chia.consensus.difficulty_adjustment import get_next_sub_slot_iters_and_difficulty
from chia.consensus.make_sub_epoch_summary import next_sub_epoch_summary
from chia.consensus.multiprocess_validation import PreValidationResult, pre_validate_block
from chia.consensus.pot_iterations import calculate_sp_iters
from chia.consensus.signage_point import SignagePoint
from chia.full_node.block_store import BlockStore
from chia.full_node.check_fork_next_block import check_fork_next_block
from chia.full_node.coin_store import CoinStore
from chia.full_node.full_node_api import FullNodeAPI
from chia.full_node.full_node_store import FullNodeStore, FullNodeStorePeakResult, UnfinishedBlockEntry
from chia.full_node.hint_management import get_hints_and_subscription_coin_ids
from chia.full_node.hint_store import HintStore
from chia.full_node.mempool import MempoolRemoveInfo
from chia.full_node.mempool_manager import MempoolManager
from chia.full_node.subscriptions import PeerSubscriptions, peers_for_spend_bundle
from chia.full_node.sync_store import Peak, SyncStore
from chia.full_node.tx_processing_queue import PeerWithTx, TransactionQueue, TransactionQueueEntry
from chia.full_node.weight_proof import WeightProofHandler
from chia.protocols import farmer_protocol, full_node_protocol, timelord_protocol, wallet_protocol
from chia.protocols.farmer_protocol import SignagePointSourceData, SPSubSlotSourceData, SPVDFSourceData
from chia.protocols.full_node_protocol import RequestBlocks, RespondBlock, RespondBlocks, RespondSignagePoint
from chia.protocols.outbound_message import Message, NodeType, make_msg
from chia.protocols.protocol_message_types import ProtocolMessageTypes
from chia.protocols.protocol_timing import CONSENSUS_ERROR_BAN_SECONDS
from chia.protocols.shared_protocol import Capability
from chia.protocols.wallet_protocol import CoinStateUpdate, RemovedMempoolItem
from chia.rpc.rpc_server import StateChangedProtocol
from chia.server.node_discovery import FullNodePeers
from chia.server.server import ChiaServer
from chia.server.ws_connection import WSChiaConnection
from chia.types.blockchain_format.classgroup import ClassgroupElement
from chia.types.blockchain_format.vdf import CompressibleVDFField, VDFInfo, VDFProof, validate_vdf
from chia.types.coin_record import CoinRecord
from chia.types.mempool_inclusion_status import MempoolInclusionStatus
from chia.types.mempool_item import MempoolItem
from chia.types.peer_info import PeerInfo
from chia.types.validation_state import ValidationState
from chia.types.weight_proof import WeightProof
from chia.util.bech32m import encode_puzzle_hash
from chia.util.config import process_config_start_method
from chia.util.db_synchronous import db_synchronous_on
from chia.util.db_version import lookup_db_version, set_db_version_async
from chia.util.db_wrapper import DBWrapper2, manage_connection
from chia.util.errors import ConsensusError, Err, TimestampError, ValidationError
from chia.util.limited_semaphore import LimitedSemaphore
from chia.util.network import is_localhost
from chia.util.path import path_from_root
from chia.util.profiler import enable_profiler, mem_profile_task, profile_task
from chia.util.safe_cancel_task import cancel_task_safe
from chia.util.task_referencer import create_referenced_task


# This is the result of calling peak_post_processing, which is then fed into peak_post_processing_2
@dataclasses.dataclass
class PeakPostProcessingResult:
    # The added transactions IDs from calling MempoolManager.new_peak
    mempool_peak_added_tx_ids: list[bytes32]
    mempool_removals: list[MempoolRemoveInfo]  # The removed mempool items from calling MempoolManager.new_peak
    fns_peak_result: FullNodeStorePeakResult  # The result of calling FullNodeStore.new_peak
    hints: list[tuple[bytes32, bytes]]  # The hints added to the DB
    lookup_coin_ids: list[bytes32]  # The coin IDs that we need to look up to notify wallets of changes
    signage_points: list[tuple[RespondSignagePoint, WSChiaConnection, Optional[EndOfSubSlotBundle]]]


@dataclasses.dataclass(frozen=True)
class WalletUpdate:
    fork_height: uint32
    peak: Peak
    coin_records: list[CoinRecord]
    hints: dict[bytes32, bytes32]


@final
@dataclasses.dataclass
class FullNode:
    if TYPE_CHECKING:
        from chia.rpc.rpc_server import RpcServiceProtocol

        _protocol_check: ClassVar[RpcServiceProtocol] = cast("FullNode", None)

    root_path: Path
    config: dict[str, Any]
    constants: ConsensusConstants
    signage_point_times: list[float]
    full_node_store: FullNodeStore
    log: logging.Logger
    db_path: Path
    wallet_sync_queue: asyncio.Queue[WalletUpdate]
    _segment_task_list: list[asyncio.Task[None]] = dataclasses.field(default_factory=list)
    initialized: bool = False
    _server: Optional[ChiaServer] = None
    _shut_down: bool = False
    pow_creation: dict[bytes32, asyncio.Event] = dataclasses.field(default_factory=dict)
    state_changed_callback: Optional[StateChangedProtocol] = None
    full_node_peers: Optional[FullNodePeers] = None
    sync_store: SyncStore = dataclasses.field(default_factory=SyncStore)
    uncompact_task: Optional[asyncio.Task[None]] = None
    compact_vdf_requests: set[bytes32] = dataclasses.field(default_factory=set)
    # TODO: Logging isn't setup yet so the log entries related to parsing the
    #       config would end up on stdout if handled here.
    multiprocessing_context: Optional[BaseContext] = None
    _ui_tasks: set[asyncio.Task[None]] = dataclasses.field(default_factory=set)
    subscriptions: PeerSubscriptions = dataclasses.field(default_factory=PeerSubscriptions)
    _transaction_queue_task: Optional[asyncio.Task[None]] = None
    simulator_transaction_callback: Optional[Callable[[bytes32], Awaitable[None]]] = None
    _sync_task_list: list[asyncio.Task[None]] = dataclasses.field(default_factory=list)
    _transaction_queue: Optional[TransactionQueue] = None
    _tx_task_list: list[asyncio.Task[None]] = dataclasses.field(default_factory=list)
    _compact_vdf_sem: Optional[LimitedSemaphore] = None
    _new_peak_sem: Optional[LimitedSemaphore] = None
    _add_transaction_semaphore: Optional[asyncio.Semaphore] = None
    _db_wrapper: Optional[DBWrapper2] = None
    _hint_store: Optional[HintStore] = None
    _block_store: Optional[BlockStore] = None
    _coin_store: Optional[CoinStoreProtocol] = None
    _mempool_manager: Optional[MempoolManager] = None
    _init_weight_proof: Optional[asyncio.Task[None]] = None
    _blockchain: Optional[Blockchain] = None
    _timelord_lock: Optional[asyncio.Lock] = None
    weight_proof_handler: Optional[WeightProofHandler] = None
    # hashes of peaks that failed long sync on chip13 Validation
    bad_peak_cache: dict[bytes32, uint32] = dataclasses.field(default_factory=dict)
    wallet_sync_task: Optional[asyncio.Task[None]] = None
    _bls_cache: BLSCache = dataclasses.field(default_factory=lambda: BLSCache(50000))

    @property
    def server(self) -> ChiaServer:
        # This is a stop gap until the class usage is refactored such the values of
        # integral attributes are known at creation of the instance.
        if self._server is None:
            raise RuntimeError("server not assigned")

        return self._server

    @classmethod
    async def create(
        cls,
        config: dict[str, Any],
        root_path: Path,
        consensus_constants: ConsensusConstants,
        name: str = __name__,
    ) -> FullNode:
        # NOTE: async to force the queue creation to occur when an event loop is available
        db_path_replaced: str = config["database_path"].replace("CHALLENGE", config["selected_network"])
        db_path = path_from_root(root_path, db_path_replaced)
        db_path.parent.mkdir(parents=True, exist_ok=True)

        return cls(
            root_path=root_path,
            config=config,
            constants=consensus_constants,
            signage_point_times=[time.time() for _ in range(consensus_constants.NUM_SPS_SUB_SLOT)],
            full_node_store=FullNodeStore(consensus_constants),
            log=logging.getLogger(name),
            db_path=db_path,
            wallet_sync_queue=asyncio.Queue(),
        )

    @contextlib.asynccontextmanager
    async def manage(self) -> AsyncIterator[None]:
        self._timelord_lock = asyncio.Lock()
        self._compact_vdf_sem = LimitedSemaphore.create(active_limit=4, waiting_limit=20)

        # We don't want to run too many concurrent new_peak instances, because it would fetch the same block from
        # multiple peers and re-validate.
        self._new_peak_sem = LimitedSemaphore.create(active_limit=2, waiting_limit=20)

        # These many respond_transaction tasks can be active at any point in time
        self._add_transaction_semaphore = asyncio.Semaphore(200)

        sql_log_path: Optional[Path] = None
        with contextlib.ExitStack() as exit_stack:
            sql_log_file: Optional[TextIO] = None
            if self.config.get("log_sqlite_cmds", False):
                sql_log_path = path_from_root(self.root_path, "log/sql.log")
                self.log.info(f"logging SQL commands to {sql_log_path}")
                sql_log_file = exit_stack.enter_context(sql_log_path.open("a", encoding="utf-8"))

            # create the store (db) and full node instance
            # TODO: is this standardized and thus able to be handled by DBWrapper2?
            async with manage_connection(self.db_path, log_file=sql_log_file, name="version_check") as db_connection:
                db_version = await lookup_db_version(db_connection)

        self.log.info(f"using blockchain database {self.db_path}, which is version {db_version}")

        db_sync = db_synchronous_on(self.config.get("db_sync", "auto"))
        self.log.info(f"opening blockchain DB: synchronous={db_sync}")

        async with DBWrapper2.managed(
            self.db_path,
            db_version=db_version,
            reader_count=self.config.get("db_readers", 4),
            log_path=sql_log_path,
            synchronous=db_sync,
        ) as self._db_wrapper:
            if self.db_wrapper.db_version != 2:
                async with self.db_wrapper.reader_no_transaction() as conn:
                    async with conn.execute(
                        "SELECT name FROM sqlite_master WHERE type='table' AND name='full_blocks'"
                    ) as cur:
                        if len(list(await cur.fetchall())) == 0:
                            try:
                                # this is a new DB file. Make it v2
                                async with self.db_wrapper.writer_maybe_transaction() as w_conn:
                                    await set_db_version_async(w_conn, 2)
                                    self.db_wrapper.db_version = 2
                                    self.log.info("blockchain database is empty, configuring as v2")
                            except sqlite3.OperationalError:
                                # it could be a database created with "chia init", which is
                                # empty except it has the database_version table
                                pass

            self._block_store = await BlockStore.create(self.db_wrapper)
            self._hint_store = await HintStore.create(self.db_wrapper)
            self._coin_store = await CoinStore.create(self.db_wrapper)
            self.log.info("Initializing blockchain from disk")
            start_time = time.monotonic()
            reserved_cores = self.config.get("reserved_cores", 0)
            single_threaded = self.config.get("single_threaded", False)
            log_coins = self.config.get("log_coins", False)
            multiprocessing_start_method = process_config_start_method(config=self.config, log=self.log)
            self.multiprocessing_context = multiprocessing.get_context(method=multiprocessing_start_method)
            selected_network = self.config.get("selected_network")
            height_map = await BlockHeightMap.create(self.db_path.parent, self._db_wrapper, selected_network)
            self._blockchain = await Blockchain.create(
                coin_store=self.coin_store,
                block_store=self.block_store,
                consensus_constants=self.constants,
                height_map=height_map,
                reserved_cores=reserved_cores,
                single_threaded=single_threaded,
                log_coins=log_coins,
            )

            self._mempool_manager = MempoolManager(
                get_coin_records=self.coin_store.get_coin_records,
                get_unspent_lineage_info_for_puzzle_hash=self.coin_store.get_unspent_lineage_info_for_puzzle_hash,
                consensus_constants=self.constants,
                single_threaded=single_threaded,
            )

            # Transactions go into this queue from the server, and get sent to respond_transaction
            self._transaction_queue = TransactionQueue(1000, self.log)
            self._transaction_queue_task: asyncio.Task[None] = create_referenced_task(self._handle_transactions())

            self._init_weight_proof = create_referenced_task(self.initialize_weight_proof())

            if self.config.get("enable_profiler", False):
                create_referenced_task(profile_task(self.root_path, "node", self.log), known_unreferenced=True)

            self.profile_block_validation = self.config.get("profile_block_validation", False)
            if self.profile_block_validation:  # pragma: no cover
                # this is not covered by any unit tests as it's essentially test code
                # itself. It's exercised manually when investigating performance issues
                profile_dir = path_from_root(self.root_path, "block-validation-profile")
                profile_dir.mkdir(parents=True, exist_ok=True)

            if self.config.get("enable_memory_profiler", False):
                create_referenced_task(mem_profile_task(self.root_path, "node", self.log), known_unreferenced=True)

            time_taken = time.monotonic() - start_time
            peak: Optional[BlockRecord] = self.blockchain.get_peak()
            if peak is None:
                self.log.info(f"Initialized with empty blockchain time taken: {int(time_taken)}s")
                if not await self.coin_store.is_empty():
                    self.log.error(
                        "Inconsistent blockchain DB file! Could not find peak block but found some coins! "
                        "This is a fatal error. The blockchain database may be corrupt"
                    )
                    raise RuntimeError("corrupt blockchain DB")
            else:
                self.log.info(
                    f"Blockchain initialized to peak {peak.header_hash} height"
                    f" {peak.height}, "
                    f"time taken: {int(time_taken)}s"
                )
                async with self.blockchain.priority_mutex.acquire(priority=BlockchainMutexPriority.high):
                    pending_tx = await self.mempool_manager.new_peak(self.blockchain.get_tx_peak(), None)
                    # No pending transactions when starting up
                    assert len(pending_tx.spend_bundle_ids) == 0

                    full_peak: Optional[FullBlock] = await self.blockchain.get_full_peak()
                    assert full_peak is not None
                    state_change_summary = StateChangeSummary(peak, uint32(max(peak.height - 1, 0)), [], [], [], [])
                    # Must be called under priority_mutex
                    ppp_result: PeakPostProcessingResult = await self.peak_post_processing(
                        full_peak, state_change_summary, None
                    )
                # Can be called outside of priority_mutex
                await self.peak_post_processing_2(full_peak, None, state_change_summary, ppp_result)
            if self.config["send_uncompact_interval"] != 0:
                sanitize_weight_proof_only = False
                if "sanitize_weight_proof_only" in self.config:
                    sanitize_weight_proof_only = self.config["sanitize_weight_proof_only"]
                assert self.config["target_uncompact_proofs"] != 0
                self.uncompact_task = create_referenced_task(
                    self.broadcast_uncompact_blocks(
                        self.config["send_uncompact_interval"],
                        self.config["target_uncompact_proofs"],
                        sanitize_weight_proof_only,
                    )
                )
            if self.wallet_sync_task is None or self.wallet_sync_task.done():
                self.wallet_sync_task = create_referenced_task(self._wallets_sync_task_handler())

            self.initialized = True

            try:
                async with contextlib.AsyncExitStack() as aexit_stack:
                    if self.full_node_peers is not None:
                        await aexit_stack.enter_async_context(self.full_node_peers.manage())
                    yield
            finally:
                self._shut_down = True
                if self._init_weight_proof is not None:
                    self._init_weight_proof.cancel()

                # blockchain is created in _start and in certain cases it may not exist here during _close
                if self._blockchain is not None:
                    self.blockchain.shut_down()
                # same for mempool_manager
                if self._mempool_manager is not None:
                    self.mempool_manager.shut_down()
                if self.uncompact_task is not None:
                    self.uncompact_task.cancel()
                if self._transaction_queue_task is not None:
                    self._transaction_queue_task.cancel()
                cancel_task_safe(task=self.wallet_sync_task, log=self.log)
                for one_tx_task in self._tx_task_list:
                    if not one_tx_task.done():
                        cancel_task_safe(task=one_tx_task, log=self.log)
                for one_sync_task in self._sync_task_list:
                    if not one_sync_task.done():
                        cancel_task_safe(task=one_sync_task, log=self.log)
                for segment_task in self._segment_task_list:
                    cancel_task_safe(segment_task, self.log)
                for task_id, task in list(self.full_node_store.tx_fetch_tasks.items()):
                    cancel_task_safe(task, self.log)
                if self._init_weight_proof is not None:
                    await asyncio.wait([self._init_weight_proof])
                for one_tx_task in self._tx_task_list:
                    if one_tx_task.done():
                        self.log.info(f"TX task {one_tx_task.get_name()} done")
                    else:
                        with contextlib.suppress(asyncio.CancelledError):
                            self.log.info(f"Awaiting TX task {one_tx_task.get_name()}")
                            await one_tx_task
                for one_sync_task in self._sync_task_list:
                    if one_sync_task.done():
                        self.log.info(f"Long sync task {one_sync_task.get_name()} done")
                    else:
                        with contextlib.suppress(asyncio.CancelledError):
                            self.log.info(f"Awaiting long sync task {one_sync_task.get_name()}")
                            await one_sync_task
                await asyncio.gather(*self._segment_task_list, return_exceptions=True)

    @property
    def block_store(self) -> BlockStore:
        assert self._block_store is not None
        return self._block_store

    @property
    def timelord_lock(self) -> asyncio.Lock:
        assert self._timelord_lock is not None
        return self._timelord_lock

    @property
    def mempool_manager(self) -> MempoolManager:
        assert self._mempool_manager is not None
        return self._mempool_manager

    @property
    def blockchain(self) -> Blockchain:
        assert self._blockchain is not None
        return self._blockchain

    @property
    def coin_store(self) -> CoinStoreProtocol:
        assert self._coin_store is not None
        return self._coin_store

    @property
    def add_transaction_semaphore(self) -> asyncio.Semaphore:
        assert self._add_transaction_semaphore is not None
        return self._add_transaction_semaphore

    @property
    def transaction_queue(self) -> TransactionQueue:
        assert self._transaction_queue is not None
        return self._transaction_queue

    @property
    def db_wrapper(self) -> DBWrapper2:
        assert self._db_wrapper is not None
        return self._db_wrapper

    @property
    def hint_store(self) -> HintStore:
        assert self._hint_store is not None
        return self._hint_store

    @property
    def new_peak_sem(self) -> LimitedSemaphore:
        assert self._new_peak_sem is not None
        return self._new_peak_sem

    @property
    def compact_vdf_sem(self) -> LimitedSemaphore:
        assert self._compact_vdf_sem is not None
        return self._compact_vdf_sem

    def get_connections(self, request_node_type: Optional[NodeType]) -> list[dict[str, Any]]:
        connections = self.server.get_connections(request_node_type)
        con_info: list[dict[str, Any]] = []
        if self.sync_store is not None:
            peak_store = self.sync_store.peer_to_peak
        else:
            peak_store = None
        for con in connections:
            if peak_store is not None and con.peer_node_id in peak_store:
                peak = peak_store[con.peer_node_id]
                peak_height = peak.height
                peak_hash = peak.header_hash
                peak_weight = peak.weight
            else:
                peak_height = None
                peak_hash = None
                peak_weight = None
            con_dict: dict[str, Any] = {
                "type": con.connection_type,
                "local_port": con.local_port,
                "peer_host": con.peer_info.host,
                "peer_port": con.peer_info.port,
                "peer_server_port": con.peer_server_port,
                "node_id": con.peer_node_id,
                "creation_time": con.creation_time,
                "bytes_read": con.bytes_read,
                "bytes_written": con.bytes_written,
                "last_message_time": con.last_message_time,
                "peak_height": peak_height,
                "peak_weight": peak_weight,
                "peak_hash": peak_hash,
            }
            con_info.append(con_dict)

        return con_info

    def _set_state_changed_callback(self, callback: StateChangedProtocol) -> None:
        self.state_changed_callback = callback

    async def _handle_one_transaction(self, entry: TransactionQueueEntry) -> None:
        peer = entry.peer
        try:
            inc_status, err = await self.add_transaction(
                entry.transaction, entry.spend_name, peer, entry.test, entry.peers_with_tx
            )
            entry.done.set((inc_status, err))
        except asyncio.CancelledError:
            error_stack = traceback.format_exc()
            self.log.debug(f"Cancelling _handle_one_transaction, closing: {error_stack}")
        except ValidationError as e:
            self.log.exception("ValidationError in _handle_one_transaction, closing")
            if peer is not None:
                await peer.close(CONSENSUS_ERROR_BAN_SECONDS)
            entry.done.set((MempoolInclusionStatus.FAILED, e.code))
        except Exception:
            self.log.exception("Error in _handle_one_transaction, closing")
            if peer is not None:
                await peer.close(CONSENSUS_ERROR_BAN_SECONDS)
            entry.done.set((MempoolInclusionStatus.FAILED, Err.UNKNOWN))
        finally:
            self.add_transaction_semaphore.release()

    async def _handle_transactions(self) -> None:
        while not self._shut_down:
            # We use a semaphore to make sure we don't send more than 200 concurrent calls of respond_transaction.
            # However, doing them one at a time would be slow, because they get sent to other processes.
            await self.add_transaction_semaphore.acquire()

            # Clean up task reference list (used to prevent gc from killing running tasks)
            for oldtask in self._tx_task_list[:]:
                if oldtask.done():
                    self._tx_task_list.remove(oldtask)

            item: TransactionQueueEntry = await self.transaction_queue.pop()
            self._tx_task_list.append(create_referenced_task(self._handle_one_transaction(item)))

    async def initialize_weight_proof(self) -> None:
        self.weight_proof_handler = WeightProofHandler(
            constants=self.constants,
            blockchain=self.blockchain,
            multiprocessing_context=self.multiprocessing_context,
        )
        peak = self.blockchain.get_peak()
        if peak is not None:
            await self.weight_proof_handler.create_sub_epoch_segments()

    def set_server(self, server: ChiaServer) -> None:
        self._server = server
        dns_servers: list[str] = []
        network_name = self.config["selected_network"]
        try:
            default_port = self.config["network_overrides"]["config"][network_name]["default_full_node_port"]
        except Exception:
            self.log.info("Default port field not found in config.")
            default_port = None
        if "dns_servers" in self.config:
            dns_servers = self.config["dns_servers"]
        elif network_name == "mainnet":
            # If `dns_servers` is missing from the `config`, hardcode it if we're running mainnet.
            dns_servers.append("dns-introducer.chia.net")
        try:
            self.full_node_peers = FullNodePeers(
                server=self.server,
                target_outbound_count=self.config["target_outbound_peer_count"],
                peers_file_path=self.root_path / Path(self.config.get("peers_file_path", "db/peers.dat")),
                introducer_info=self.config["introducer_peer"],
                dns_servers=dns_servers,
                peer_connect_interval=self.config["peer_connect_interval"],
                selected_network=self.config["selected_network"],
                default_port=default_port,
                log=self.log,
            )
        except Exception as e:
            error_stack = traceback.format_exc()
            self.log.error(f"Exception: {e}")
            self.log.error(f"Exception in peer discovery: {e}")
            self.log.error(f"Exception Stack: {error_stack}")

    def _state_changed(self, change: str, change_data: Optional[dict[str, Any]] = None) -> None:
        if self.state_changed_callback is not None:
            self.state_changed_callback(change, change_data)

    async def short_sync_batch(self, peer: WSChiaConnection, start_height: uint32, target_height: uint32) -> bool:
        """
        Tries to sync to a chain which is not too far in the future, by downloading batches of blocks. If the first
        block that we download is not connected to our chain, we return False and do an expensive long sync instead.
        Long sync is not preferred because it requires downloading and validating a weight proof.

        Args:
            peer: peer to sync from
            start_height: height that we should start downloading at. (Our peak is higher)
            target_height: target to sync to

        Returns:
            False if the fork point was not found, and we need to do a long sync. True otherwise.

        """
        # Don't trigger multiple batch syncs to the same peer

        if self.sync_store.is_backtrack_syncing(node_id=peer.peer_node_id):
            return True  # Don't batch sync, we are already in progress of a backtrack sync
        if peer.peer_node_id in self.sync_store.batch_syncing:
            return True  # Don't trigger a long sync
        self.sync_store.batch_syncing.add(peer.peer_node_id)

        self.log.info(f"Starting batch short sync from {start_height} to height {target_height}")
        if start_height > 0:
            first = await peer.call_api(
                FullNodeAPI.request_block, full_node_protocol.RequestBlock(uint32(start_height), False)
            )
            if first is None or not isinstance(first, full_node_protocol.RespondBlock):
                self.sync_store.batch_syncing.remove(peer.peer_node_id)
                self.log.error(f"Error short batch syncing, could not fetch block at height {start_height}")
                return False
            hash = self.blockchain.height_to_hash(first.block.height - 1)
            assert hash is not None
            if hash != first.block.prev_header_hash:
                self.log.info("Batch syncing stopped, this is a deep chain")
                self.sync_store.batch_syncing.remove(peer.peer_node_id)
                # First sb not connected to our blockchain, do a long sync instead
                return False

        batch_size = self.constants.MAX_BLOCK_COUNT_PER_REQUESTS
        for task in self._segment_task_list[:]:
            if task.done():
                self._segment_task_list.remove(task)
            else:
                cancel_task_safe(task=task, log=self.log)

        try:
            peer_info = peer.get_peer_logging()
            if start_height > 0:
                fork_hash = self.blockchain.height_to_hash(uint32(start_height - 1))
            else:
                fork_hash = self.constants.GENESIS_CHALLENGE
            assert fork_hash
            fork_info = ForkInfo(start_height - 1, start_height - 1, fork_hash)
            blockchain = AugmentedBlockchain(self.blockchain)
            for height in range(start_height, target_height, batch_size):
                end_height = min(target_height, height + batch_size)
                request = RequestBlocks(uint32(height), uint32(end_height), True)
                response = await peer.call_api(FullNodeAPI.request_blocks, request)
                if not response:
                    raise ValueError(f"Error short batch syncing, invalid/no response for {height}-{end_height}")
                async with self.blockchain.priority_mutex.acquire(priority=BlockchainMutexPriority.high):
                    state_change_summary: Optional[StateChangeSummary]
                    prev_b = None
                    if response.blocks[0].height > 0:
                        prev_b = await self.blockchain.get_block_record_from_db(response.blocks[0].prev_header_hash)
                        assert prev_b is not None
                    new_slot = len(response.blocks[0].finished_sub_slots) > 0
                    ssi, diff = get_next_sub_slot_iters_and_difficulty(
                        self.constants, new_slot, prev_b, self.blockchain
                    )
                    vs = ValidationState(ssi, diff, None)
                    success, state_change_summary = await self.add_block_batch(
                        response.blocks, peer_info, fork_info, vs, blockchain
                    )
                    if not success:
                        raise ValueError(f"Error short batch syncing, failed to validate blocks {height}-{end_height}")
                    if state_change_summary is not None:
                        try:
                            peak_fb: Optional[FullBlock] = await self.blockchain.get_full_peak()
                            assert peak_fb is not None
                            ppp_result: PeakPostProcessingResult = await self.peak_post_processing(
                                peak_fb,
                                state_change_summary,
                                peer,
                            )
                        except Exception:
                            # Still do post processing after cancel (or exception)
                            peak_fb = await self.blockchain.get_full_peak()
                            assert peak_fb is not None
                            await self.peak_post_processing(peak_fb, state_change_summary, peer)
                            raise
                        finally:
                            self.log.info(f"Added blocks {height}-{end_height}")
                if state_change_summary is not None and peak_fb is not None:
                    # Call outside of priority_mutex to encourage concurrency
                    await self.peak_post_processing_2(peak_fb, peer, state_change_summary, ppp_result)
        finally:
            self.sync_store.batch_syncing.remove(peer.peer_node_id)
        return True

    async def short_sync_backtrack(
        self, peer: WSChiaConnection, peak_height: uint32, target_height: uint32, target_unf_hash: bytes32
    ) -> bool:
        """
        Performs a backtrack sync, where blocks are downloaded one at a time from newest to oldest. If we do not
        find the fork point 5 deeper than our peak, we return False and do a long sync instead.

        Args:
            peer: peer to sync from
            peak_height: height of our peak
            target_height: target height
            target_unf_hash: partial hash of the unfinished block of the target

        Returns:
            True iff we found the fork point, and we do not need to long sync.
        """
        try:
            self.sync_store.increment_backtrack_syncing(node_id=peer.peer_node_id)

            unfinished_block: Optional[UnfinishedBlock] = self.full_node_store.get_unfinished_block(target_unf_hash)
            curr_height: int = target_height
            found_fork_point = False
            blocks = []
            while curr_height > peak_height - 5:
                # If we already have the unfinished block, don't fetch the transactions. In the normal case, we will
                # already have the unfinished block, from when it was broadcast, so we just need to download the header,
                # but not the transactions
                fetch_tx: bool = unfinished_block is None or curr_height != target_height
                curr = await peer.call_api(
                    FullNodeAPI.request_block, full_node_protocol.RequestBlock(uint32(curr_height), fetch_tx)
                )
                if curr is None:
                    raise ValueError(f"Failed to fetch block {curr_height} from {peer.get_peer_logging()}, timed out")
                if curr is None or not isinstance(curr, full_node_protocol.RespondBlock):
                    raise ValueError(
                        f"Failed to fetch block {curr_height} from {peer.get_peer_logging()}, wrong type {type(curr)}"
                    )
                blocks.append(curr.block)
                if curr_height == 0:
                    found_fork_point = True
                    break
                hash_at_height = self.blockchain.height_to_hash(curr.block.height - 1)
                if hash_at_height is not None and hash_at_height == curr.block.prev_header_hash:
                    found_fork_point = True
                    break
                curr_height -= 1
            if found_fork_point:
                first_block = blocks[-1]  # blocks are reveresd this is the lowest block to add
                # we create the fork_info and pass it here so it would be updated on each call to add_block
                fork_info = ForkInfo(first_block.height - 1, first_block.height - 1, first_block.prev_header_hash)
                for block in reversed(blocks):
                    # when syncing, we won't share any signatures with the
                    # mempool, so there's no need to pass in the BLS cache.
                    await self.add_block(block, peer, fork_info=fork_info)
        except (asyncio.CancelledError, Exception):
            self.sync_store.decrement_backtrack_syncing(node_id=peer.peer_node_id)
            raise

        self.sync_store.decrement_backtrack_syncing(node_id=peer.peer_node_id)
        return found_fork_point

    async def _refresh_ui_connections(self, sleep_before: float = 0) -> None:
        if sleep_before > 0:
            await asyncio.sleep(sleep_before)
        self._state_changed("peer_changed_peak")

    async def new_peak(self, request: full_node_protocol.NewPeak, peer: WSChiaConnection) -> None:
        """
        We have received a notification of a new peak from a peer. This happens either when we have just connected,
        or when the peer has updated their peak.

        Args:
            request: information about the new peak
            peer: peer that sent the message

        """

        try:
            seen_header_hash = self.sync_store.seen_header_hash(request.header_hash)
            # Updates heights in the UI. Sleeps 1.5s before, so other peers have time to update their peaks as well.
            # Limit to 3 refreshes.
            if not seen_header_hash and len(self._ui_tasks) < 3:
                self._ui_tasks.add(create_referenced_task(self._refresh_ui_connections(1.5)))
            # Prune completed connect tasks
            self._ui_tasks = set(filter(lambda t: not t.done(), self._ui_tasks))
        except Exception as e:
            self.log.warning(f"Exception UI refresh task: {e}")

        # Store this peak/peer combination in case we want to sync to it, and to keep track of peers
        self.sync_store.peer_has_block(request.header_hash, peer.peer_node_id, request.weight, request.height, True)

        if self.blockchain.contains_block(request.header_hash, request.height):
            return None

        # Not interested in less heavy peaks
        peak: Optional[BlockRecord] = self.blockchain.get_peak()
        curr_peak_height = uint32(0) if peak is None else peak.height
        if peak is not None and peak.weight > request.weight:
            return None

        if self.sync_store.get_sync_mode():
            # If peer connects while we are syncing, check if they have the block we are syncing towards
            target_peak = self.sync_store.target_peak
            if target_peak is not None and request.header_hash != target_peak.header_hash:
                peak_peers: set[bytes32] = self.sync_store.get_peers_that_have_peak([target_peak.header_hash])
                # Don't ask if we already know this peer has the peak
                if peer.peer_node_id not in peak_peers:
                    target_peak_response: Optional[RespondBlock] = await peer.call_api(
                        FullNodeAPI.request_block,
                        full_node_protocol.RequestBlock(target_peak.height, False),
                        timeout=10,
                    )
                    if (
                        target_peak_response is not None
                        and isinstance(target_peak_response, RespondBlock)
                        and target_peak_response.block.header_hash == target_peak.header_hash
                    ):
                        self.sync_store.peer_has_block(
                            target_peak.header_hash,
                            peer.peer_node_id,
                            target_peak_response.block.weight,
                            target_peak.height,
                            False,
                        )
        else:
            if (
                curr_peak_height <= request.height
                and request.height <= curr_peak_height + self.config["short_sync_blocks_behind_threshold"]
            ):
                # This is the normal case of receiving the next block
                if await self.short_sync_backtrack(
                    peer, curr_peak_height, request.height, request.unfinished_reward_block_hash
                ):
                    return None

            if request.height < self.constants.WEIGHT_PROOF_RECENT_BLOCKS:
                # This is the case of syncing up more than a few blocks, at the start of the chain
                self.log.debug("Doing batch sync, no backup")
                await self.short_sync_batch(peer, uint32(0), request.height)
                return None

            if (
                curr_peak_height <= request.height
                and request.height < curr_peak_height + self.config["sync_blocks_behind_threshold"]
            ):
                # This case of being behind but not by so much
                if await self.short_sync_batch(peer, uint32(max(curr_peak_height - 6, 0)), request.height):
                    return None

            # Clean up task reference list (used to prevent gc from killing running tasks)
            for oldtask in self._sync_task_list[:]:
                if oldtask.done():
                    self._sync_task_list.remove(oldtask)

            # This is the either the case where we were not able to sync successfully (for example, due to the fork
            # point being in the past), or we are very far behind. Performs a long sync.
            # Multiple tasks may be created here. If we don't save all handles, a task could enter a sync object
            # and be cleaned up by the GC, corrupting the sync object and possibly not allowing anything else in.
            self._sync_task_list.append(create_referenced_task(self._sync()))

    async def send_peak_to_timelords(
        self, peak_block: Optional[FullBlock] = None, peer: Optional[WSChiaConnection] = None
    ) -> None:
        """
        Sends current peak to timelords
        """
        if peak_block is None:
            peak_block = await self.blockchain.get_full_peak()
        if peak_block is not None:
            peak = self.blockchain.block_record(peak_block.header_hash)
            difficulty = self.blockchain.get_next_sub_slot_iters_and_difficulty(peak.header_hash, False)[1]
            ses: Optional[SubEpochSummary] = next_sub_epoch_summary(
                self.constants,
                self.blockchain,
                peak.required_iters,
                peak_block,
                True,
            )
            recent_rc = self.blockchain.get_recent_reward_challenges()

            curr = peak
            while not curr.is_challenge_block(self.constants) and not curr.first_in_sub_slot:
                curr = self.blockchain.block_record(curr.prev_hash)

            if curr.is_challenge_block(self.constants):
                last_csb_or_eos = curr.total_iters
            else:
                last_csb_or_eos = curr.ip_sub_slot_total_iters(self.constants)

            curr = peak
            passed_ses_height_but_not_yet_included = True
            while (curr.height % self.constants.SUB_EPOCH_BLOCKS) != 0:
                if curr.sub_epoch_summary_included:
                    passed_ses_height_but_not_yet_included = False
                curr = self.blockchain.block_record(curr.prev_hash)
            if curr.sub_epoch_summary_included or curr.height == 0:
                passed_ses_height_but_not_yet_included = False

            timelord_new_peak: timelord_protocol.NewPeakTimelord = timelord_protocol.NewPeakTimelord(
                peak_block.reward_chain_block,
                difficulty,
                peak.deficit,
                peak.sub_slot_iters,
                ses,
                recent_rc,
                last_csb_or_eos,
                passed_ses_height_but_not_yet_included,
            )

            msg = make_msg(ProtocolMessageTypes.new_peak_timelord, timelord_new_peak)
            if peer is None:
                await self.server.send_to_all([msg], NodeType.TIMELORD)
            else:
                await self.server.send_to_specific([msg], peer.peer_node_id)

    async def synced(self, block_is_current_at: Optional[uint64] = None) -> bool:
        if block_is_current_at is None:
            block_is_current_at = uint64(time.time() - 60 * 7)
        if "simulator" in str(self.config.get("selected_network")):
            return True  # sim is always synced because it has no peers
        curr: Optional[BlockRecord] = self.blockchain.get_peak()
        if curr is None:
            return False

        while curr is not None and not curr.is_transaction_block:
            curr = self.blockchain.try_block_record(curr.prev_hash)

        if (
            curr is None
            or curr.timestamp is None
            or curr.timestamp < block_is_current_at
            or self.sync_store.get_sync_mode()
        ):
            return False
        else:
            return True

    async def on_connect(self, connection: WSChiaConnection) -> None:
        """
        Whenever we connect to another node / wallet, send them our current heads. Also send heads to farmers
        and challenges to timelords.
        """

        self._state_changed("add_connection")
        self._state_changed("sync_mode")
        # TODO: this can probably be improved
        if self.full_node_peers is not None:
            create_referenced_task(self.full_node_peers.on_connect(connection))

        if self.initialized is False:
            return None

        if connection.connection_type is NodeType.FULL_NODE:
            # Send filter to node and request mempool items that are not in it (Only if we are currently synced)
            synced = await self.synced()
            peak_height = self.blockchain.get_peak_height()
            if synced and peak_height is not None:
                my_filter = self.mempool_manager.get_filter()
                mempool_request = full_node_protocol.RequestMempoolTransactions(my_filter)

                msg = make_msg(ProtocolMessageTypes.request_mempool_transactions, mempool_request)
                await connection.send_message(msg)

        peak_full: Optional[FullBlock] = await self.blockchain.get_full_peak()

        if peak_full is not None:
            peak: BlockRecord = self.blockchain.block_record(peak_full.header_hash)
            if connection.connection_type is NodeType.FULL_NODE:
                request_node = full_node_protocol.NewPeak(
                    peak.header_hash,
                    peak.height,
                    peak.weight,
                    peak.height,
                    peak_full.reward_chain_block.get_unfinished().get_hash(),
                )
                await connection.send_message(make_msg(ProtocolMessageTypes.new_peak, request_node))

            elif connection.connection_type is NodeType.WALLET:
                # If connected to a wallet, send the Peak
                request_wallet = wallet_protocol.NewPeakWallet(
                    peak.header_hash,
                    peak.height,
                    peak.weight,
                    peak.height,
                )
                await connection.send_message(make_msg(ProtocolMessageTypes.new_peak_wallet, request_wallet))
            elif connection.connection_type is NodeType.TIMELORD:
                await self.send_peak_to_timelords()

    async def on_disconnect(self, connection: WSChiaConnection) -> None:
        self.log.info(f"peer disconnected {connection.get_peer_logging()}")
        self._state_changed("close_connection")
        self._state_changed("sync_mode")
        if self.sync_store is not None:
            self.sync_store.peer_disconnected(connection.peer_node_id)
        # Remove all ph | coin id subscription for this peer
        self.subscriptions.remove_peer(connection.peer_node_id)

    async def _sync(self) -> None:
        """
        Performs a full sync of the blockchain up to the peak.
            - Wait a few seconds for peers to send us their peaks
            - Select the heaviest peak, and request a weight proof from a peer with that peak
            - Validate the weight proof, and disconnect from the peer if invalid
            - Find the fork point to see where to start downloading blocks
            - Download blocks in batch (and in parallel) and verify them one at a time
            - Disconnect peers that provide invalid blocks or don't have the blocks
        """
        # Ensure we are only syncing once and not double calling this method
        fork_point: Optional[uint32] = None
        if self.sync_store.get_sync_mode():
            return None

        if self.sync_store.get_long_sync():
            self.log.debug("already in long sync")
            return None

        self.sync_store.set_long_sync(True)
        self.log.debug("long sync started")
        try:
            self.log.info("Starting to perform sync.")

            # Wait until we have 3 peaks or up to a max of 30 seconds
            max_iterations = int(self.config.get("max_sync_wait", 30)) * 10

            self.log.info(f"Waiting to receive peaks from peers. (timeout: {max_iterations / 10}s)")
            peaks = []
            for i in range(max_iterations):
                peaks = [peak.header_hash for peak in self.sync_store.get_peak_of_each_peer().values()]
                if len(self.sync_store.get_peers_that_have_peak(peaks)) < 3:
                    if self._shut_down:
                        return None
                    await asyncio.sleep(0.1)
                    continue
                break

            self.log.info(f"Collected a total of {len(peaks)} peaks.")

            # Based on responses from peers about the current peaks, see which peak is the heaviest
            # (similar to longest chain rule).
            target_peak = self.sync_store.get_heaviest_peak()

            if target_peak is None:
                raise RuntimeError("Not performing sync, no peaks collected")

            self.sync_store.target_peak = target_peak

            self.log.info(f"Selected peak {target_peak}")
            # Check which peers are updated to this height

            peers = self.server.get_connections(NodeType.FULL_NODE)
            coroutines = []
            for peer in peers:
                coroutines.append(
                    peer.call_api(
                        FullNodeAPI.request_block,
                        full_node_protocol.RequestBlock(target_peak.height, True),
                        timeout=10,
                    )
                )
            for i, target_peak_response in enumerate(await asyncio.gather(*coroutines)):
                if (
                    target_peak_response is not None
                    and isinstance(target_peak_response, RespondBlock)
                    and target_peak_response.block.header_hash == target_peak.header_hash
                ):
                    self.sync_store.peer_has_block(
                        target_peak.header_hash, peers[i].peer_node_id, target_peak.weight, target_peak.height, False
                    )
            # TODO: disconnect from peer which gave us the heaviest_peak, if nobody has the peak
            fork_point, summaries = await self.request_validate_wp(
                target_peak.header_hash, target_peak.height, target_peak.weight
            )
            # Ensures that the fork point does not change
            async with self.blockchain.priority_mutex.acquire(priority=BlockchainMutexPriority.high):
                await self.blockchain.warmup(fork_point)
                fork_point = await check_fork_next_block(
                    self.blockchain,
                    fork_point,
                    self.get_peers_with_peak(target_peak.header_hash),
                    node_next_block_check,
                )
                await self.sync_from_fork_point(fork_point, target_peak.height, target_peak.header_hash, summaries)
        except asyncio.CancelledError:
            self.log.warning("Syncing failed, CancelledError")
        except Exception as e:
            tb = traceback.format_exc()
            self.log.error(f"Error with syncing: {type(e)}{tb}")
        finally:
            if self._shut_down:
                return None
            await self._finish_sync(fork_point)

    async def request_validate_wp(
        self, peak_header_hash: bytes32, peak_height: uint32, peak_weight: uint128
    ) -> tuple[uint32, list[SubEpochSummary]]:
        if self.weight_proof_handler is None:
            raise RuntimeError("Weight proof handler is None")
        peers_with_peak = self.get_peers_with_peak(peak_header_hash)
        # Request weight proof from a random peer
        peers_with_peak_len = len(peers_with_peak)
        self.log.info(f"Total of {peers_with_peak_len} peers with peak {peak_height}")
        # We can't choose from an empty sequence
        if peers_with_peak_len == 0:
            raise RuntimeError(f"Not performing sync, no peers with peak {peak_height}")
        weight_proof_peer: WSChiaConnection = random.choice(peers_with_peak)
        self.log.info(
            f"Requesting weight proof from peer {weight_proof_peer.peer_info.host} up to height {peak_height}"
        )
        cur_peak: Optional[BlockRecord] = self.blockchain.get_peak()
        if cur_peak is not None and peak_weight <= cur_peak.weight:
            raise ValueError("Not performing sync, already caught up.")
        wp_timeout = 360
        if "weight_proof_timeout" in self.config:
            wp_timeout = self.config["weight_proof_timeout"]
        self.log.debug(f"weight proof timeout is {wp_timeout} sec")
        request = full_node_protocol.RequestProofOfWeight(peak_height, peak_header_hash)
        response = await weight_proof_peer.call_api(FullNodeAPI.request_proof_of_weight, request, timeout=wp_timeout)
        # Disconnect from this peer, because they have not behaved properly
        if response is None or not isinstance(response, full_node_protocol.RespondProofOfWeight):
            await weight_proof_peer.close(CONSENSUS_ERROR_BAN_SECONDS)
            raise RuntimeError(f"Weight proof did not arrive in time from peer: {weight_proof_peer.peer_info.host}")
        if response.wp.recent_chain_data[-1].reward_chain_block.height != peak_height:
            await weight_proof_peer.close(CONSENSUS_ERROR_BAN_SECONDS)
            raise RuntimeError(f"Weight proof had the wrong height: {weight_proof_peer.peer_info.host}")
        if response.wp.recent_chain_data[-1].reward_chain_block.weight != peak_weight:
            await weight_proof_peer.close(CONSENSUS_ERROR_BAN_SECONDS)
            raise RuntimeError(f"Weight proof had the wrong weight: {weight_proof_peer.peer_info.host}")
        if self.in_bad_peak_cache(response.wp):
            raise ValueError("Weight proof failed bad peak cache validation")
        # dont sync to wp if local peak is heavier,
        # dont ban peer, we asked for this peak
        current_peak = self.blockchain.get_peak()
        if current_peak is not None:
            if response.wp.recent_chain_data[-1].reward_chain_block.weight <= current_peak.weight:
                raise RuntimeError(
                    f"current peak is heavier than Weight proof peek: {weight_proof_peer.peer_info.host}"
                )
        try:
            validated, fork_point, summaries = await self.weight_proof_handler.validate_weight_proof(response.wp)
        except Exception as e:
            await weight_proof_peer.close(CONSENSUS_ERROR_BAN_SECONDS)
            raise ValueError(f"Weight proof validation threw an error {e}")
        if not validated:
            await weight_proof_peer.close(CONSENSUS_ERROR_BAN_SECONDS)
            raise ValueError("Weight proof validation failed")
        self.log.info(f"Re-checked peers: total of {len(peers_with_peak)} peers with peak {peak_height}")
        self.sync_store.set_sync_mode(True)
        self._state_changed("sync_mode")
        return fork_point, summaries

    async def sync_from_fork_point(
        self,
        fork_point_height: uint32,
        target_peak_sb_height: uint32,
        peak_hash: bytes32,
        summaries: list[SubEpochSummary],
    ) -> None:
        self.log.info(f"Start syncing from fork point at {fork_point_height} up to {target_peak_sb_height}")
        batch_size = self.constants.MAX_BLOCK_COUNT_PER_REQUESTS
        counter = 0
        if fork_point_height != 0:
            # warmup the cache
            curr = self.blockchain.height_to_block_record(fork_point_height)
            while (
                curr.sub_epoch_summary_included is None
                or counter < 3 * self.constants.MAX_SUB_SLOT_BLOCKS + self.constants.MIN_BLOCKS_PER_CHALLENGE_BLOCK + 3
            ):
                res = await self.blockchain.get_block_record_from_db(curr.prev_hash)
                if res is None:
                    break
                curr = res
                self.blockchain.add_block_record(curr)
                counter += 1

        # normally "fork_point" or "fork_height" refers to the first common
        # block between the main chain and the fork. Here "fork_point_height"
        # seems to refer to the first diverging block
        # in case we're validating a reorg fork (i.e. not extending the
        # main chain), we need to record the coin set from that fork in
        # fork_info. Otherwise validation is very expensive, especially
        # for deep reorgs
        if fork_point_height > 0:
            fork_hash = self.blockchain.height_to_hash(uint32(fork_point_height - 1))
            assert fork_hash is not None
        else:
            fork_hash = self.constants.GENESIS_CHALLENGE
        fork_info = ForkInfo(fork_point_height - 1, fork_point_height - 1, fork_hash)

        if fork_point_height == 0:
            ssi = self.constants.SUB_SLOT_ITERS_STARTING
            diff = self.constants.DIFFICULTY_STARTING
            prev_ses_block = None
        else:
            prev_b_hash = self.blockchain.height_to_hash(fork_point_height)
            assert prev_b_hash is not None
            prev_b = await self.blockchain.get_full_block(prev_b_hash)
            assert prev_b is not None
            ssi, diff, prev_ses_block = await self.get_sub_slot_iters_difficulty_ses_block(prev_b, None, None)

        # we need an augmented blockchain to validate blocks in batches. The
        # batch must be treated as if it's part of the chain to validate the
        # blocks in it. We also need them to keep appearing as if they're part
        # of the chain when pipelining the validation of blocks. We start
        # validating the next batch while still adding the first batch to the
        # chain.
        blockchain = AugmentedBlockchain(self.blockchain)
        peers_with_peak: list[WSChiaConnection] = self.get_peers_with_peak(peak_hash)

        async def fetch_blocks(output_queue: asyncio.Queue[Optional[tuple[WSChiaConnection, list[FullBlock]]]]) -> None:
            # the rate limit for respond_blocks is 100 messages / 60 seconds.
            # But the limit is scaled to 30% for outbound messages, so that's 30
            # messages per 60 seconds.
            # That's 2 seconds per request.
            seconds_per_request = 2
            start_height, end_height = 0, 0

            # the timestamp of when the next request_block message is allowed to
            # be sent. It's initialized to the current time, and bumped by the
            # seconds_per_request every time we send a request. This ensures we
            # won't exceed the 100 requests / 60 seconds rate limit.
            # Whichever peer has the lowest timestamp is the one we request
            # from. peers that take more than 5 seconds to respond are pushed to
            # the end of the queue, to be less likely to request from.

            # This should be cleaned up to not be a hard coded value, and maybe
            # allow higher request rates (and align the request_blocks and
            # respond_blocks rate limits).
            now = time.monotonic()
            new_peers_with_peak: list[tuple[WSChiaConnection, float]] = [(c, now) for c in peers_with_peak[:]]
            self.log.info(f"peers with peak: {len(new_peers_with_peak)}")
            random.shuffle(new_peers_with_peak)
            try:
                # block request ranges are *inclusive*, this requires some
                # gymnastics of this range (+1 to make it exclusive, like normal
                # ranges) and then -1 when forming the request message
                for start_height in range(fork_point_height, target_peak_sb_height + 1, batch_size):
                    end_height = min(target_peak_sb_height, start_height + batch_size - 1)
                    request = RequestBlocks(uint32(start_height), uint32(end_height), True)
                    new_peers_with_peak.sort(key=lambda pair: pair[1])
                    fetched = False
                    for idx, (peer, timestamp) in enumerate(new_peers_with_peak):
                        if peer.closed:
                            continue

                        start = time.monotonic()
                        if start < timestamp:
                            # rate limit ourselves, since we sent a message to
                            # this peer too recently
                            await asyncio.sleep(timestamp - start)
                            start = time.monotonic()

                        # update the timestamp, now that we're sending a request
                        # it's OK for the timestamp to fall behind wall-clock
                        # time. It just means we're allowed to send more
                        # requests to catch up
                        if is_localhost(peer.peer_info