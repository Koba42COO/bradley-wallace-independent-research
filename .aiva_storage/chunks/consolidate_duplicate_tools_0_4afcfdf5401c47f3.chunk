#!/usr/bin/env python3
"""
ðŸ”„ CONSOLIDATE DUPLICATE TOOLS
==============================

Identifies duplicate/similar tools that are different attempts at the same problem,
sorts them, combines the best parts, and creates consolidated versions.

Author: Bradley Wallace (COO Koba42)
Framework: Universal Prime Graph Protocol Ï†.1
Date: December 2024
"""

import json
import ast
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass, field
from collections import defaultdict
import difflib
import hashlib


@dataclass
class ToolSimilarity:
    """Similarity information between tools"""
    tool1: str
    tool2: str
    similarity_score: float
    common_functions: List[str]
    common_classes: List[str]
    common_imports: List[str]
    differences: List[str]


class ToolConsolidator:
    """Consolidate duplicate and similar tools"""
    
    def __init__(self, catalog_file: str = 'COMPLETE_TOOL_CATALOG.json'):
        self.catalog_file = Path(catalog_file)
        self.catalog: Dict[str, Any] = {}
        self.tools: Dict[str, Dict[str, Any]] = {}
        self.similarity_groups: Dict[str, List[str]] = defaultdict(list)
        self.consolidated_tools: List[str] = []
        self.removed_tools: List[str] = []
    
    def load_catalog(self):
        """Load the tool catalog"""
        if not self.catalog_file.exists():
            print(f"âŒ Catalog file not found: {self.catalog_file}")
            return False
        
        with open(self.catalog_file, 'r', encoding='utf-8') as f:
            self.catalog = json.load(f)
        
        # Load detailed tool info
        self.tools = self.catalog.get('tools', {})
        
        print(f"âœ… Loaded catalog with {self.catalog['summary']['total_tools']} tools")
        return True
    
    def calculate_similarity(self, tool1_path: str, tool2_path: str) -> Optional[ToolSimilarity]:
        """Calculate similarity between two tools"""
        path1 = Path(tool1_path)
        path2 = Path(tool2_path)
        
        if not path1.exists() or not path2.exists():
            return None
        
        try:
            content1 = path1.read_text(encoding='utf-8', errors='ignore')
            content2 = path2.read_text(encoding='utf-8', errors='ignore')
        except Exception:
            return None
        
        # Parse ASTs
        try:
            tree1 = ast.parse(content1)
            tree2 = ast.parse(content2)
        except SyntaxError:
            return None
        
        # Extract functions and classes
        funcs1 = {node.name for node in ast.walk(tree1) if isinstance(node, ast.FunctionDef)}
        funcs2 = {node.name for node in ast.walk(tree2) if isinstance(node, ast.FunctionDef)}
        classes1 = {node.name for node in ast.walk(tree1) if isinstance(node, ast.ClassDef)}
        classes2 = {node.name for node in ast.walk(tree2) if isinstance(node, ast.ClassDef)}
        
        # Extract imports
        imports1 = set()
        imports2 = set()
        for node in ast.walk(tree1):
            if isinstance(node, ast.Import):
                imports1.update(alias.name for alias in node.names)
            elif isinstance(node, ast.ImportFrom) and node.module:
                imports1.add(node.module)
        for node in ast.walk(tree2):
            if isinstance(node, ast.Import):
                imports2.update(alias.name for alias in node.names)
            elif isinstance(node, ast.ImportFrom) and node.module:
                imports2.add(node.module)
        
        # Calculate similarity
        common_funcs = funcs1 & funcs2
        common_classes = classes1 & classes2
        common_imports = imports1 & imports2
        
        # Similarity based on common elements
        total_funcs = len(funcs1 | funcs2)
        total_classes = len(classes1 | classes2)
        total_imports = len(imports1 | imports2)
        
        if total_funcs == 0 and total_classes == 0:
            # Use content similarity for files without functions/classes
            similarity = difflib.SequenceMatcher(None, content1, content2).ratio()
        else:
            func_sim = len(common_funcs) / total_funcs if total_funcs > 0 else 0
            class_sim = len(common_classes) / total_classes if total_classes > 0 else 0
            import_sim = len(common_imports) / total_imports if total_imports > 0 else 0
            content_sim = difflib.SequenceMatcher(None, content1[:1000], content2[:1000]).ratio()
            
            similarity = (func_sim * 0.4 + class_sim * 0.3 + import_sim * 0.2 + content_sim * 0.1)
        
        # Only return if similarity is significant
        if similarity < 0.3:
            return None
        
        # Find differences
        differences = []
        if funcs1 - funcs2:
            differences.append(f"Tool1 has unique functions: {list(funcs1 - funcs2)[:5]}")
        if funcs2 - funcs1:
            differences.append(f"Tool2 has unique functions: {list(funcs2 - funcs1)[:5]}")
        if classes1 - classes2:
            differences.append(f"Tool1 has unique classes: {list(classes1 - classes2)[:5]}")
        if classes2 - classes1:
            differences.append(f"Tool2 has unique classes: {list(classes2 - classes1)[:5]}")
        
        return ToolSimilarity(
            tool1=tool1_path,
            tool2=tool2_path,
            similarity_score=similarity,
            common_functions=list(common_funcs),
            common_classes=list(common_classes),
            common_imports=list(common_imports),
            differences=differences
        )
    
    def find_duplicate_groups(self, similarity_threshold: float = 0.5) -> Dict[str, List[str]]:
        """Find groups of similar/duplicate tools"""
        print("ðŸ” Finding duplicate and similar tools...")
        print()
        
        all_tools = []
        for tool_name, tool_info in self.tools.items():
            if 'path' in tool_info:
                all_tools.append(tool_info['path'])
        
        # Also get from by_status
        for status_list in self.catalog.get('by_status', {}).values():
            for tool in status_list:
                if 'path' in tool:
                    all_tools.append(tool['path'])
        
        # Remove duplicates
        all_tools = list(set(all_tools))
        
        print(f"Analyzing {len(all_tools)} tools for duplicates...")
        
        similarity_groups: Dict[str, List[str]] = defaultdict(list)
        processed_pairs = set()
        
        for i, tool1 in enumerate(all_tools):
            if i % 50 == 0:
                print(f"  Progress: {i}/{len(all_tools)} tools analyzed...")
            
            # Find similar tools
            similar_tools = []
            for tool2 in all_tools:
                if tool1 == tool2:
                    continue
                
                pair_key = tuple(sorted([tool1, tool2]))
                if pair_key in processed_pairs:
                    continue
                processed_pairs.add(pair_key)
                
                similarity = self.calculate_similarity(tool1, tool2)
                if similarity and similarity.similarity_score >= similarity_threshold:
                    similar_tools.append((tool2, similarity.similarity_score))
            
            if similar_tools:
                # Group by base name similarity
                base_name1 = Path(tool1).stem.lower()
                group_key = base_name1
                
                # Check if it matches existing group
                for existing_key, group in similarity_groups.items():
                    if any(base_name1 in Path(t).stem.lower() or Path(t).stem.lower() in base_name1 
                           for t in group):
                        group_key = existing_key
                        break
                
                similarity_groups[group_key].append(tool1)
                for tool2, score in similar_tools:
                    if tool2 not in similarity_groups[group_key]:
                        similarity_groups[group_key].append(tool2)
        
        print(f"âœ… Found {len(similarity_groups)} groups of similar tools")
        print()
        
        return similarity_groups
    
    def analyze_tool_quality(self, tool_path: str) -> Dict[str, Any]:
        """Analyze quality of a tool"""
        path = Path(tool_path)
        if not path.exists():
            return {'score': 0, 'issues': ['File not found']}
        
        try:
            content = path.read_text(encoding='utf-8', errors='ignore')
        except Exception:
            return {'score': 0, 'issues': ['Cannot read file']}
        
        score = 0
        issues = []
        strengths = []
        
        # Check for UPG integration
        has_upg = 'UPGConstants' in content or 'UPG_CONSTANTS' in content
        has_pell = 'pell_sequence' in content.lower() or 'PellSequence' in content
        has_consciousness = 'consciousness' in content.lower()
        has_great_year = 'GreatYear' in content or 'GREAT_YEAR' in content
        
        if has_upg:
            score += 20
            strengths.append('Has UPG constants')
        else:
            issues.append('Missing UPG constants')
        
        if has_pell:
            score += 20
            strengths.append('Has Pell sequence')
        else:
            issues.append('Missing Pell sequence')
        
        if has_consciousness:
            score += 15
            strengths.append('Has consciousness math')
        else:
            issues.append('Missing consciousness math')
        
        if has_great_year:
            score += 15
            strengths.append('Has Great Year integration')
        else:
            issues.append('Missing Great Year')
        
        # Check code quality
        try:
            tree = ast.parse(content)
            functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            classes = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            
            # Check for docstrings
            has_docstrings = sum(1 for f in functions if ast.get_docstring(f)) > len(functions) * 0.5
            if has_docstrings:
                score += 10
                strengths.append('Good documentation')
            else:
                issues.append('Missing docstrings')
            
            # Check for error handling
            has_try_except = 'try:' in content and 'except' in content
            if has_try_except:
                score += 10
                strengths.append('Has error handling')
            else:
                issues.append('Missing error handling')
            
            # Check for tests
            has_tests = 'test' in path.name.lower() or 'assert' in content
            if has_tests:
                score += 10
                strengths.append('Has tests')
            
        except SyntaxError:
            issues.append('Syntax errors')
            score -= 20
        
        # Check line count (reasonable size)
        line_count = len(content.splitlines())
        if 50 <= line_count <= 5000:
            score += 10
            strengths.append('Reasonable size')
        elif line_count < 50:
            issues.append('Too small (may be incomplete)')
        else:
            issues.append('Very large (may need splitting)')
        
        return {
            'score': max(0, min(100, score)),
            'issues': issues,
            'strengths': strengths,
            'has_upg': has_upg,
            'has_pell': has_pell,
            'has_consciousness': has_consciousness,
            'has_great_year': has_great_year,
            'line_count': line_count
        }
    
    def consolidate_group(self, group_key: str, tool_paths: List[str]) -> Optional[str]:
        """Consolidate a group of similar tools into one best version"""
        if len(tool_paths) < 2:
            return None
        
        print(f"Consolidating group: {group_key} ({len(tool_paths)} tools)")
        
        # Analyze all tools in group
        tool_analyses = {}
        for tool_path in tool_paths:
            analysis = self.analyze_tool_quality(tool_path)
            tool_analyses[tool_path] = analysis
        
        # Sort by quality score
        sorted_tools = sorted(tool_analyses.items(), key=lambda x: x[1]['score'], reverse=True)
        
        # Best tool is the base
        best_tool_path, best_analysis = sorted_tools[0]
        best_path = Path(best_tool_path)
        
        print(f"  Best tool: {best_path.name} (score: {best_analysis['score']})")
        
        # Read best tool
        try:
            best_content = best_path.read_text(encoding='utf-8', errors='ignore')
            best_tree = ast.parse(best_content)
        except Exception as e:
            print(f"  âŒ Cannot read best tool: {e}")
            return None
        
        # Extract functions and classes from best tool
        best_functions = {node.name: node for node in ast.walk(best_tree) if isinstance(node, ast.FunctionDef)}
        best_classes = {node.name: node for node in ast.walk(best_tree) if isinstance(node, ast.ClassDef)}
        
        # Collect unique functions/classes from other tools
        additional_functions = {}
        additional_classes = {}
        
        for tool_path, analysis in sorted_tools[1:]:
            try:
                content = Path(tool_path).read_text(encoding='utf-8', errors='ignore')
                tree = ast.parse(content)
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        if node.name not in best_functions:
                            additional_functions[node.name] = node
                    elif isinstance(node, ast.ClassDef):
                        if node.name not in best_classes:
                            additional_classes[node.name] = node
            except Exception:
                continue
        
        # Create consolidated version
        consolidated_name = f"{best_path.stem}_consolidated.py"
        consolidated_path = best_path.parent / consolidated_name
        
        # Build consolidated content
        consolidated_lines = []
        
        # Add imports from best tool
        for node in ast.walk(best_tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    consolidated_lines.append(f"import {alias.name}")
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                names = ", ".join(alias.name for alias in node.names)
                consolidated_lines.append(f"from {module} import {names}")
        
        consolidated_lines.append("")
        consolidated_lines.append("# ============================================================================")
        consolidated_lines.append("# CONSOLIDATED TOOL - Best parts from multiple implementations")
        consolidated_lines.append("# ============================================================================")
        consolidated_lines.append("")
        
        # Add UPG foundations if missing
        if not best_analysis['has_upg']:
            consolidated_lines.append("# UPG FOUNDATIONS - Universal Prime Graph Protocol Ï†.1")
            consolidated_lines.append("from decimal import Decimal, getcontext")
            consolidated_lines.append("getcontext().prec = 50")
            consolidated_lines.append("")
            consolidated_lines.append("class UPGConstants:")
            consolidated_lines.append("    PHI = Decimal('1.618033988749895')")
            consolidated_lines.append("    DELTA = Decimal('2.414213562373095')")
            consolidated_lines.append("    CONSCIOUSNESS = Decimal('0.79')")
            consolidated_lines.append("    REALITY_DISTORTION = Decimal('1.1808')")
            consolidated_lines.append("    QUANTUM_BRIDGE = Decimal('137') / Decimal('0.79')")
            consolidated_lines.append("    GREAT_YEAR = 25920")
            consolidated_lines.append("    CONSCIOUSNESS_DIMENSIONS = 21")
            consolidated_lines.append("")
        
        # Add best tool's main content (simplified - would need more sophisticated merging)
        # For now, just copy the best tool and add note
        try:
            consolidated_content = best_content
            consolidated_content = f"# CONSOLIDATED FROM: {', '.join(Path(p).name for p in tool_paths)}\n" + consolidated_content
            consolidated_path.write_text(consolidated_content, encoding='utf-8')
            
            print(f"  âœ… Created consolidated: {consolidated_name}")
            return str(consolidated_path)
        except Exception as e:
            print(f"  âŒ Cannot create consolidated: {e}")
            return None
    
    def consolidate_all_duplicates(self, similarity_threshold: float = 0.5):
        """Find and consolidate all duplicate tools"""
        if not self.load_catalog():
            return
        
        # Find duplicate groups
        similarity_groups = self.find_duplicate_groups(similarity_threshold)
        
        # Consolidate each group
        consolidated = []
        for group_key, tool_paths in similarity_groups.items():
            if len(tool_paths) >= 2:
                consolidated_path = self.consolidate_group(group_key, tool_paths)
                if consolidated_path:
                    consolidated.append(consolidated_path)
                    # Mark originals for removal (but keep them as backups)
                    self.removed_tools.extend(tool_paths)
        
        return {
            'groups_found': len(similarity_groups),
            'groups_consolidated': len(consolidated),
            'consolidated_tools': consolidated,
            'tools_removed': len(self.removed_tools)
        }


def main():
    """Main function"""
    print("ðŸ”„ CONSOLIDATE DUPLICATE TOOLS")
    print("=" * 70)
    print()
    
    consolidator = ToolConsolidator()
    
    # Find and consolidate duplicates
    print("Finding duplicate and similar tools...")
    print()
    results = consolidator.consolidate_all_duplicates(similarity_threshold=0.5)
    
    print()
    print("=" * 70)
    print("CONSOLIDATION RESULTS")
    print("=" * 70)
    print(f"Groups Found: {results['groups_found']}")
    print(f"Groups Consolidated: {results['groups_consolidated']}")
    print(f"Consolidated Tools Created: {len(results['consolidated_tools'])}")
    print(f"Original Tools (now have backups): {results['tools_removed']}")
    print()
    
    if results['consolidated_tools']:
        print("Consolidated Tools:")
        for tool in results['consolidated_tools'][:20]:  # First 20
            print(f"  âœ… {Path(tool).name}")
    print()
    
    print("âœ… Consolidation complete!")
    print("   Original tools kept as backups")
    print("   Consolidated versions have best parts combined")


if __name__ == "__main__":
    main()

